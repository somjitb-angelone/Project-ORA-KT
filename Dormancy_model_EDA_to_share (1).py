# Databricks notebook source
displayHTML("""<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="809px" height="645px" viewBox="-0.5 -0.5 809 645" content="&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2022-04-27T17:00:26.422Z&quot; agent=&quot;5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&quot; etag=&quot;J610CIg0aMPjoUXgT8z2&quot; version=&quot;17.4.6&quot; type=&quot;google&quot;&gt;&lt;diagram id=&quot;L0iLOLPCXRMgT5qkfgFM&quot; name=&quot;Page-1&quot;&gt;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&lt;/diagram&gt;&lt;/mxfile&gt;" style="background-color: rgb(255, 255, 255);"><defs><style type="text/css">@import url(https://fonts.googleapis.com/css?family=Architects+Daughter);&#xa;</style></defs><g><path d="M 74 315 L 106 315 L 131.63 314.6" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 136.88 314.52 L 129.94 318.13 L 131.63 314.6 L 129.83 311.13 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 44 285 L 44 232.37" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 44 227.12 L 47.5 234.12 L 44 232.37 L 40.5 234.12 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><ellipse cx="44" cy="315" rx="30" ry="30" fill="#cce5ff" stroke="#36393d" pointer-events="all"/><image x="22.5" y="305.5" width="18" height="18" xlink:href="https://cdn1.iconfinder.com/data/icons/iconoir-vol-4/24/twitter-verified-badge-128.png" preserveAspectRatio="none"/><rect x="25" y="300" width="60" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 315px; margin-left: 26px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>KYC</b></div></div></div></foreignObject><text x="55" y="319" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">KYC</text></switch></g><path d="M 203 314.5 L 231.5 314.5 L 253.63 314.89" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 258.88 314.98 L 251.82 318.36 L 253.63 314.89 L 251.94 311.36 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 170.5 282 L 170.5 235.37" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 170.5 230.12 L 174 237.12 L 170.5 235.37 L 167 237.12 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><ellipse cx="170.5" cy="314.5" rx="32.5" ry="32.5" fill="#cdeb8b" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 315px; margin-left: 139px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>#sauda is 1</b></div></div></div></foreignObject><text x="171" y="318" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">#sauda is 1</text></switch></g><path d="M 292.5 282.5 L 292.5 235.87" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 292.5 230.62 L 296 237.62 L 292.5 235.87 L 289 237.62 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 325 315 L 383.63 315" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 388.88 315 L 381.88 318.5 L 383.63 315 L 381.88 311.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><ellipse cx="292.5" cy="315" rx="32.5" ry="32.5" fill="#ffcccc" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 315px; margin-left: 261px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>#sauda is &lt; 4</b></div></div></div></foreignObject><text x="293" y="319" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">#sauda is &lt;...</text></switch></g><ellipse cx="513.5" cy="318.5" rx="32.5" ry="32.5" fill="#ffcc99" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 319px; margin-left: 482px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>Q-r</b></div></div></div></foreignObject><text x="514" y="322" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Q-r</text></switch></g><ellipse cx="513.5" cy="417.5" rx="32.5" ry="32.5" fill="#ffcc99" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 418px; margin-left: 482px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>W-r</b></div></div></div></foreignObject><text x="514" y="421" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">W-r</text></switch></g><ellipse cx="592.5" cy="487.5" rx="32.5" ry="32.5" fill="#ffcc99" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 488px; margin-left: 561px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>M-r</b></div></div></div></foreignObject><text x="593" y="491" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">M-r</text></switch></g><ellipse cx="429.5" cy="487.5" rx="32.5" ry="32.5" fill="#ffcc99" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 488px; margin-left: 398px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>BW-r</b></div></div></div></foreignObject><text x="430" y="491" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">BW-r</text></switch></g><ellipse cx="44" cy="196" rx="30" ry="30" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 58px; height: 1px; padding-top: 196px; margin-left: 15px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>Dormant</b></div></div></div></foreignObject><text x="44" y="200" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Dormant</text></switch></g><ellipse cx="170.5" cy="196.5" rx="32.5" ry="32.5" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 197px; margin-left: 139px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>Dormant</b></div></div></div></foreignObject><text x="171" y="200" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Dormant</text></switch></g><ellipse cx="292.5" cy="197" rx="32.5" ry="32.5" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 197px; margin-left: 261px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>Dormant</b></div></div></div></foreignObject><text x="293" y="201" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Dormant</text></switch></g><path d="M 513.5 287 L 513.5 237.37" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 513.5 232.12 L 517 239.12 L 513.5 237.37 L 510 239.12 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><ellipse cx="513.5" cy="198.5" rx="32.5" ry="32.5" fill="rgb(255, 255, 255)" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 63px; height: 1px; padding-top: 199px; margin-left: 482px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>Dormant</b></div></div></div></foreignObject><text x="514" y="202" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Dormant</text></switch></g><path d="M 390 565 L 390 285" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 630 565 L 630 282" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 630 565 L 390 565" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 632.5 285 L 392.5 285" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 431.75 449.04 L 478.75 324.46" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 429.89 453.95 L 429.09 446.17 L 431.75 449.04 L 435.64 448.64 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 480.61 319.55 L 481.41 327.33 L 478.75 324.46 L 474.86 324.86 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 548.05 324.53 L 590.45 448.97" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 546.36 319.56 L 551.93 325.06 L 548.05 324.53 L 545.3 327.31 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 592.14 453.94 L 586.57 448.44 L 590.45 448.97 L 593.2 446.19 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 468.37 487.5 L 553.63 487.5" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 463.12 487.5 L 470.12 484 L 468.37 487.5 L 470.12 491 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 558.88 487.5 L 551.88 491 L 553.63 487.5 L 551.88 484 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 513.59 378.63 L 513.91 357.37" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 513.52 383.88 L 510.12 376.83 L 513.59 378.63 L 517.12 376.93 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 513.98 352.12 L 517.38 359.17 L 513.91 357.37 L 510.38 359.07 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 455.45 461.7 L 485.07 443.78" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 450.96 464.42 L 455.13 457.8 L 455.45 461.7 L 458.76 463.79 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 489.56 441.06 L 485.39 447.68 L 485.07 443.78 L 481.76 441.69 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 541.65 444.2 L 565.35 461.28" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 537.39 441.13 L 545.11 442.39 L 541.65 444.2 L 541.02 448.07 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 569.61 464.35 L 561.89 463.1 L 565.35 461.28 L 565.98 457.42 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><path d="M 315 155 L 315 66.37" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 315 61.12 L 318.5 68.12 L 315 66.37 L 311.5 68.12 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><rect x="0" y="155" width="630" height="90" fill-opacity="0.4" fill="#f8cecc" stroke="#b85450" stroke-opacity="0.4" pointer-events="all"/><rect x="360" y="166" width="110" height="49" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 108px; height: 1px; padding-top: 191px; margin-left: 361px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>Long and Ultra Long period of noSauda</b></div></div></div></foreignObject><text x="415" y="194" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Long and Ultra Lon...</text></switch></g><ellipse cx="315" cy="30" rx="60" ry="30" fill="none" stroke="rgb(0, 0, 0)" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 30px; margin-left: 256px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">App-Uninstall </div></div></div></foreignObject><text x="315" y="34" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">App-Uninstall </text></switch></g><rect x="385" y="286" width="100" height="20" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 296px; margin-left: 435px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: nowrap;"><b>#sauda is &gt;= 4</b></div></div></div></foreignObject><text x="435" y="300" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">#sauda is &gt;= 4</text></switch></g><rect x="241" y="436" width="150" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 148px; height: 1px; padding-top: 451px; margin-left: 242px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>Q-r</b> : Recency of last sauda ~ Quarter<br /><b>W-r</b>: Recency of sauda ~ W/D<br /><b>BW-r</b>: Recency of sauda ~ two week<br /><b>M-r</b>: Recency of sauda ~ Month</div></div></div></foreignObject><text x="316" y="455" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Q-r : Recency of last sau...</text></switch></g><path d="M 330 285 L 325 285 Q 320 285 320 295 L 320 585 Q 320 595 315 595 L 312.5 595 Q 310 595 315 595 L 317.5 595 Q 320 595 320 605 L 320 895 Q 320 905 325 905 L 330 905" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" transform="rotate(270,320,595)" pointer-events="all"/><rect x="130" y="614" width="120" height="30" fill="#cdeb8b" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 629px; margin-left: 131px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">First Sauda</div></div></div></foreignObject><text x="190" y="633" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">First Sauda</text></switch></g><rect x="250" y="614" width="140" height="30" fill="#ffcccc" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 138px; height: 1px; padding-top: 629px; margin-left: 251px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Settling-In Sauda</div></div></div></foreignObject><text x="320" y="633" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Settling-In Sauda</text></switch></g><rect x="391" y="614" width="239" height="30" fill="#ffcc99" stroke="#36393d" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 237px; height: 1px; padding-top: 629px; margin-left: 392px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;">Frequent Sauda</div></div></div></foreignObject><text x="511" y="633" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Frequent Sauda</text></switch></g><rect x="110.5" y="267.5" width="519.5" height="300" fill-opacity="0.4" fill="#d5e8d4" stroke="#82b366" stroke-opacity="0.4" pointer-events="all"/><rect x="0" y="614" width="70" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 68px; height: 1px; padding-top: 629px; margin-left: 1px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>DTU View</b></div></div></div></foreignObject><text x="35" y="633" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">DTU View</text></switch></g><path d="M 660 155 L 655 155 Q 650 155 650 165 L 650 355 Q 650 365 645 365 L 642.5 365 Q 640 365 645 365 L 647.5 365 Q 650 365 650 375 L 650 565 Q 650 575 655 575 L 660 575" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" transform="rotate(180,650,365)" pointer-events="all"/><rect x="660" y="125" width="120" height="30" fill="none" stroke="none" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 118px; height: 1px; padding-top: 140px; margin-left: 661px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: normal; overflow-wrap: normal;"><b>[FTCV] First Trade Cohort View</b></div></div></div></foreignObject><text x="720" y="144" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">[FTCV] First Trade C...</text></switch></g><rect x="670" y="215" width="20" height="30" fill="#f8cecc" stroke="#b85450" pointer-events="all"/><rect x="710" y="205" width="20" height="40" fill="#f8cecc" stroke="#b85450" pointer-events="all"/><rect x="750" y="185" width="20" height="60" fill="#f8cecc" stroke="#b85450" pointer-events="all"/><rect x="750" y="267.5" width="20" height="60" fill="#d5e8d4" stroke="#82b366" pointer-events="all"/><rect x="710" y="267.5" width="20" height="77.5" fill="#d5e8d4" stroke="#82b366" pointer-events="all"/><rect x="670" y="267.5" width="20" height="97.5" fill="#d5e8d4" stroke="#82b366" pointer-events="all"/><ellipse cx="680" cy="383.5" rx="7.5" ry="7.5" fill="#cdeb8b" stroke="#36393d" pointer-events="all"/><path d="M 680 391 L 680 416 M 680 396 L 665 396 M 680 396 L 695 396 M 680 416 L 665 436 M 680 416 L 695 436" fill="none" stroke="#36393d" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-start; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 443px; margin-left: 680px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: nowrap;">Q3-21</div></div></div></foreignObject><text x="680" y="455" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Q3-21</text></switch></g><ellipse cx="720" cy="372.5" rx="7.5" ry="7.5" fill="#ffcccc" stroke="#36393d" pointer-events="all"/><path d="M 720 380 L 720 405 M 720 385 L 705 385 M 720 385 L 735 385 M 720 405 L 705 425 M 720 405 L 735 425" fill="none" stroke="#36393d" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-start; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 432px; margin-left: 720px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: nowrap;">Q3-21</div></div></div></foreignObject><text x="720" y="444" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Q3-21</text></switch></g><ellipse cx="765" cy="354.5" rx="7.5" ry="7.5" fill="#ffcc99" stroke="#36393d" pointer-events="all"/><path d="M 765 362 L 765 387 M 765 367 L 750 367 M 765 367 L 780 367 M 765 387 L 750 407 M 765 387 L 780 407" fill="none" stroke="#36393d" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-start; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 414px; margin-left: 765px;"><div data-drawio-colors="color: rgb(0, 0, 0); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: nowrap;">Q3-21</div></div></div></foreignObject><text x="765" y="426" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="12px" text-anchor="middle">Q3-21</text></switch></g><path d="M 660 485 L 793.63 485" fill="none" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 798.88 485 L 791.88 488.5 L 793.63 485 L 791.88 481.5 Z" fill="rgb(0, 0, 0)" stroke="rgb(0, 0, 0)" stroke-miterlimit="10" pointer-events="all"/><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 486px; margin-left: 731px;"><div data-drawio-colors="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); " style="box-sizing: border-box; font-size: 0px; text-align: center;"><div style="display: inline-block; font-size: 11px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; background-color: rgb(255, 255, 255); white-space: nowrap;">-4M<br />to<br />3M</div></div></div></foreignObject><text x="731" y="489" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="11px" text-anchor="middle">-4M...</text></switch></g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-end; justify-content: unsafe flex-start; width: 1px; height: 1px; padding-top: 483px; margin-left: 662px;"><div data-drawio-colors="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); " style="box-sizing: border-box; font-size: 0px; text-align: left;"><div style="display: inline-block; font-size: 11px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; background-color: rgb(255, 255, 255); white-space: nowrap;">0:100</div></div></div></foreignObject><text x="662" y="483" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="11px">0:100</text></switch></g><g transform="translate(-0.5 -0.5)"><switch><foreignObject pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility" style="overflow: visible; text-align: left;"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe flex-end; justify-content: unsafe flex-end; width: 1px; height: 1px; padding-top: 483px; margin-left: 799px;"><div data-drawio-colors="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255); " style="box-sizing: border-box; font-size: 0px; text-align: right;"><div style="display: inline-block; font-size: 11px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; background-color: rgb(255, 255, 255); white-space: nowrap;">50:50</div></div></div></foreignObject><text x="799" y="483" fill="rgb(0, 0, 0)" font-family="Helvetica" font-size="11px" text-anchor="end">50:50</text></switch></g></g><switch><g requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"/><a transform="translate(0,-5)" xlink:href="https://www.diagrams.net/doc/faq/svg-export-text-problems" target="_blank"><text text-anchor="middle" font-size="10px" x="50%" y="100%">Text is not SVG - cannot display</text></a></switch></svg>""")

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ###Context  
# MAGIC **Dormancy to Trade Recency and Frequency**
# MAGIC 
# MAGIC **What do we mean by A1 customers going dormant?**
# MAGIC - Dormancy was being used as a continuous non-transaction-day(s) streak of >= 60 days [or 30 days]
# MAGIC **What was the state of currently deployed predictive model?**
# MAGIC - Runs at the end of week
# MAGIC - Rank-order every user in our current ~ 9.1 M customer base
# MAGIC   - based on their likelihood to complete 60 days of non-transaction streak on or before next 60 days from the date of prediction
# MAGIC - Model itself is trained on a sub population of customers onboarded in two specific months and is assumed to be geenralisable for the full population
# MAGIC **What is the desired utility of such a prediction?**
# MAGIC - If the size of dormancy client can be mitigated by design of offers and campaigns, we can target customers as per the model-rank
# MAGIC - Decreasing the dormancy ensures larger revenue
# MAGIC - While the model is tuned to be better in accurately predicting dormancy (preferrable with association signals), however the response to the stimulus is beyond the scope of the modeling exercise
# MAGIC **Why did we relook at the problem formulation and the definition of dormancy?**
# MAGIC - What is the most representative sample for training the model?
# MAGIC - How should we characterise the transaction frequency related heterogeneity in our customer-base?
# MAGIC - When is the right time for distributing stimulus for preventing dormancy-akin behavior?
# MAGIC **What are the key missing information for instituting a business decisions around prevention of dormancy transitions?**
# MAGIC 
# MAGIC | [A] How likely is a customer to become dormant? | [B] What are the likely set of stimulus that can reduce the dormancy likelihood?  |
# MAGIC | ----------- | ----------- |

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC In this notebook we have performed different Exploratory Data Analysis to understand the data and to illicit next set of actions.
# MAGIC 
# MAGIC **Data Source for exploration**
# MAGIC |Table Name|Location|
# MAGIC |--------|------|
# MAGIC |*sn_clientkyc*|*Qubole [s3]*|
# MAGIC |*As_OrderCountData*|*Qubole [s3]*|
# MAGIC |*SN_TradingDays*|*imported to databricks from revenue server*|

# COMMAND ----------

# DBTITLE 1,Importing packages
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, OneHotEncoder,MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
import warnings
warnings.filterwarnings("ignore")
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error,mean_absolute_error 
import datetime
import sys 
from datetime import timedelta
import datetime

# Set display option to view all columns/rows
pd.set_option('display.max_columns', None)  
pd.set_option('display.expand_frame_repr', False)
pd.set_option('max_colwidth', -1)

from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)


# COMMAND ----------

# DBTITLE 1,Data Load - Client KYC & Order Count
# Fetching data from sn_clientkyc table in Qubole
file_location = "s3://angel-server-data-dev/online-engine/raw/SN_ClientKYC"  
file_type = "orc"
# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","
# The applied options are for CSV files. For other file types, these will be ignored.
df_sn = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)
#Creating temporary table
df_sn.createOrReplaceTempView("SN_ClientKYC")
df_sn.unpersist()
#########################################################################################

# Fetching data from As_OrderCountData table in Qubole 
file_location = "s3://angel-server-data-dev/online-engine/raw/AS_OrderCountData"  
file_type = "orc"
# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","
# The applied options are for CSV files. For other file types, these will be ignored.
df_as = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)
df_as.createOrReplaceTempView("as_ordercountdata")
df_as.unpersist()

# COMMAND ----------

# MAGIC %md
# MAGIC ###Exploration & Insights

# COMMAND ----------

# MAGIC %md
# MAGIC ####MoM % dormancy
# MAGIC **If we group our customer base as per their first trade month and explore their MoM (>60d) dormancy percentage,** 
# MAGIC   1. Does the percentage stabilise?
# MAGIC     - What is the stability floor of percentages?
# MAGIC   2. Do we see only increasing/decreasing or a mixture in stabilisation process?
# MAGIC   3. How soon does the percentage stabilise?
# MAGIC   4. Has the stabilisation process if observed, changed over time? [In other words] Does first trade month has an association with the dormancy rate?
# MAGIC 
# MAGIC **Background**
# MAGIC 
# MAGIC For this analysis we have considered data from Dec-20 to Nov-21  
# MAGIC i.e Month over month dormancy percentage analysis of customers who activated in between Dec-20 to Nov-21. 

# COMMAND ----------

# DBTITLE 1,Function definition to calculate current dormancy percentage for customers who did their first trade between specified dates
def dormancy_eda(date1,date2,date3):
    
    #Below query fetch customers who got activated between date2 and date3 and their last trade before date1
    month_target1 = sqlContext.sql(f"""select party_code,max(sauda_date) as last_trade,min(sauda_date) as first_trade
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y') 
					           and sauda_date <= cast( '"""+date1+"""' as date)
                               group by party_code having min(sauda_date) between cast( '"""+date2+"""' as date) and cast( '"""+date3+"""' as date) """)
    
    if (month_target1.rdd.isEmpty()):
        dormancy_per = 'No data'
    else:    
        month_target=month_target1.toPandas()
        month_target1.unpersist()
        month_total = len(month_target.index)                    #Total number of activated customers
        
        month_target['last_trade']=pd.to_datetime(month_target['last_trade'])
        df_date = pd.DataFrame({'date':[date1]})
        df_date['date'] = pd.to_datetime(df_date['date'])
        dates1 = df_date['date'].values.astype('datetime64[D]')
        dates2 = month_target['last_trade'].values.astype('datetime64[D]')
        month_target['last_trade_diff'] = (dates1 -dates2)                       #Difference between laat tradea and date for which we are calculating dormancy 
        month_target['last_trade_diff'] = month_target['last_trade_diff'].dt.days
        month_target['target'] = np.where(month_target['last_trade_diff'].astype('int') <= 60 , 0, 1)
        dormant_df = month_target[month_target['target'] == 1]
        dormant_total = len(dormant_df.index)                                     #dormant customers on date1 who activated between date2 and date3 
    
        dormancy_per = round(((dormant_total/month_total)*100),2)                 #Calculating dormancy percentage on date1 
        
    return dormancy_per

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC - Dormancy percentage as observed at the end of *column-header* month for customers who did their first trade in *row-header* month
# MAGIC 
# MAGIC |x|March-21|
# MAGIC |---|----|
# MAGIC |Jan-21|**For the group of customers who did their first trade in Jan-21, what percentage is dormant at the end of march-21**|

# COMMAND ----------

# DBTITLE 1,Compute
month_list1 = ['Dec20','Jan21','Feb21','Mar21','Apr21','May21','Jun21','Jul21','Aug21','Sep21','Oct21','Nov21']
df_month_analysis = pd.DataFrame(month_list1,columns = ['Activation month'])                     #Creating first column of output dataframe 
month_list = ['Mar21','April21','May21','Jun21','Jul21','Aug21','Sep21','Oct21','Nov21','Dec21','Jan22']   #Creating first row of output dataframe

#Input list for date1 i.e date/month on which we are calculating dormancy
date1_list=['2021-03-31','2021-04-30','2021-05-31','2021-06-30','2021-07-31','2021-08-31','2021-09-30','2021-10-31','2021-11-30','2021-12-31','2022-01-31']

#Input list for date2 and date3 i.e month in which customers got activated
activation_list = [['2020-12-1','2020-12-31'],['2021-01-01','2021-01-31'],['2021-02-01','2021-02-28'],['2021-03-01','2021-03-31'],['2021-04-01','2021-04-30']
                   ,['2021-05-01','2021-05-31'],['2021-06-01','2021-06-30'],['2021-07-01','2021-07-31'],['2021-08-01','2021-08-31'],['2021-09-01','2021-09-30'],
                  ['2021-10-01','2021-10-31'],['2021-11-01','2021-11-30']]
pos = 0         #Used for creating series by taking header value from month_list    

#Using for loop creating data column by column where column represents month in which customer got dormant 
for date1 in date1_list:
    out_list = []
    for i in activation_list:
        #Running this loop only if last date of activation month and last day of dormancy month has difference greater than 60
        if datetime.datetime.strptime(date1, '%Y-%m-%d') > datetime.datetime.strptime(i[1], '%Y-%m-%d') + pd.DateOffset(days=60):
            out = dormancy_eda(date1,i[0],i[1])              #Calling function to calculate dormancy
            out_list.append(out)
        else:
            break
    month_df = pd.DataFrame(out_list,columns= [month_list[pos]])          #Created values for month month_list[pos]
    df_month_analysis[month_list[pos]] = pd.Series(out_list)
                           
    pos += 1
    
df_month_analysis.fillna('', inplace=True)                                #To remove NA values   
print (df_month_analysis)   

#1.53 hour

# COMMAND ----------

# DBTITLE 1,Compute
#outname = 'df_month_analysis.csv'
#outdir = '/dbfs/FileStore/'
#df_month_analysis.to_csv(outdir+outname, index=False, encoding="utf-8")
df_month_analysis_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/df_month_analysis.csv')
df_temp1 = df_month_analysis_csv.toPandas()
df_temp = df_temp1.replace(r'^\s*$', np.NaN, regex=True)
sparkDF=spark.createDataFrame(df_temp)
display(sparkDF)

# COMMAND ----------

# MAGIC %md
# MAGIC ######Insights
# MAGIC 
# MAGIC *(How to read above table: First value 35.35 represents customers activated in Dec-20 and got dormant in Mar-21)*
# MAGIC 
# MAGIC 
# MAGIC - Within three months of first trade date, dormancy percentage stabilised to approx 50%-50% of active-dormant customers
# MAGIC - Dormancy % increases month over month in most of the cases
# MAGIC - Once dormancy reaches 50% it increases very slowly
# MAGIC - Quality of acquired users is degrading in the given span e.g Customers in Dec-20 reaches 36% in 2 months while  customers in Nov-21 reaches 41% in 2 months.
# MAGIC - This analysis helps us to identify if the first trade has an impact on the dormancy rate. i.e within the first 3 month of the first trade customer becomes stable and once he becomes stable there are low chances of the customer getting dormant.
# MAGIC     - We can model only the recent 3 month population and rest assured that in terms of dormancy percentage their is generalisability
# MAGIC - As we understood 3 months bucket is important now we can design our further analysis for recent 3 months bucket from first trade only

# COMMAND ----------

# MAGIC %md
# MAGIC ####DoD trade freq
# MAGIC 
# MAGIC Here, carrying from the previous insight, we defined 3 types of customers:
# MAGIC ```
# MAGIC Type1: DTU for day-T who performed first trade 3 months before day-T
# MAGIC                 
# MAGIC Type2: DTU for day-T who performed first trade within 3 months of day-T
# MAGIC                 
# MAGIC Type3: DTU for day-T who performed first trade on the same day i.e day-T
# MAGIC ```
# MAGIC 
# MAGIC **If we group our daily transacting user (DTU on a given date T) basis their first trade as per three heterogenous classes, >3m, <3m and first trade,** 
# MAGIC 1. How we can define the transaction frequency behaviour of users based on T+1 to T+30 days
# MAGIC 2. Can we identify granularity at which we need to create model for retraining, if there is observed volatility?
# MAGIC 3. Is there a trend in percentage of customer of particular type on any given day-T over DTU for the same day?
# MAGIC 4. Is there an observed stable frequency (# of trading days ) of these types over next T+1 day, T+7 day(between 2 to 7 days), T+30 day (between 8 to 30 days).
# MAGIC    - Does 4 help us to regroup the observed heterogeneity for the model building puprpose
# MAGIC  

# COMMAND ----------

# DBTITLE 1,Function definition that splits DTU for a given date into their first trade type and next trade date type
def dormancy_eda2(date1):
    
    #Total party codes traded on T day
    day0_total1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y') 
					           and sauda_date = cast( '"""+date1+"""' as date) """)
    day0_total=day0_total1.toPandas()
    day0_total1.unpersist() 
    total = len(day0_total.index)
    
    
    #Out of total party code whose first trade was older than 3 months
    day0_type1_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) <= date_add(cast( '"""+date1+"""' as date),-90)""")

    day0_type1=day0_type1_1.toPandas()
    day0_type1_1.unpersist() 
    type1 = len(day0_type1.index)
    
    #Out of total party code whose first trade within 3 months
    day0_type2_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) 
                               between date_add(cast( '"""+date1+"""' as date),-89) and date_add(cast( '"""+date1+"""' as date),-1)""")

    day0_type2=day0_type2_1.toPandas()
    day0_type2_1.unpersist()     
    type2 = len(day0_type2.index)
    
    #Out of total party code whose first trade is on T day
    day0_type3_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) = cast( '"""+date1+"""' as date) """)

    day0_type3=day0_type3_1.toPandas()
    day0_type3_1.unpersist() 
    type3 = len(day0_type3.index)
    
    #Party_codes who also traded on T as well as T+1 day
    day0_t1_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                              (select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y') 
					           and sauda_date = cast( '"""+date1+"""' as date))
                               and sauda_date = date_add(cast( '"""+date1+"""' as date),1)""")
    
    day0_t1=day0_t1_1.toPandas()
    day0_t1_1.unpersist() 
    t1 = len(day0_t1.index)
    
    #Party_codes who also traded on T as well as between T+2 to T+7 day
    day0_t7_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                              (select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y') 
					           and sauda_date = cast( '"""+date1+"""' as date))
                               and sauda_date between date_add(cast( '"""+date1+"""' as date),2) and date_add(cast( '"""+date1+"""' as date),7) """)

    day0_t7=day0_t7_1.toPandas()
    day0_t7_1.unpersist() 
    t7 = len(day0_t7.index)
    
    #Party_codes who traded on T also traded between T+8 to T+30 day
    day0_t30_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                              (select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y') 
					           and sauda_date = cast( '"""+date1+"""' as date))
                               and sauda_date between date_add(cast( '"""+date1+"""' as date),8) and date_add(cast( '"""+date1+"""' as date),30) """)

    day0_t30=day0_t30_1.toPandas()
    day0_t30_1.unpersist() 
    t30 = len(day0_t30.index)
    
    print (total)
    type1_per = round(((type1/total)* 100),2)               #Type1 percentage on T day
    type2_per = round(((type2/total)* 100),2)               #Type2 percentage on T day 
    type3_per = round(((type3/total)* 100),2)               #Type3 percentage on T day 
    t1_per    = round(((t1/total)* 100),2)                  #Percentage of customers who traded on T+1 day also 
    t7_per    = round(((t7/total)* 100),2)                  #Percentage of customers who traded also beteen T+2 to T+7 day 
    t30_per   = round(((t30/total)* 100),2)                 #Percentage of customers who traded also beteen T+8 to T+30 day  

    
    return type1_per,type2_per,type3_per,t1_per,t7_per,t30_per

# COMMAND ----------

# MAGIC %md
# MAGIC - Example of table : Percentage based on Type and next trade date
# MAGIC 
# MAGIC |Date|Type-1%|Type-2%|Type-3%|T+1%|T+2 to T+7%|T+8 to T+30%|
# MAGIC |---|----|----|----|----|----|----|
# MAGIC |2021-11-15||

# COMMAND ----------

# DBTITLE 1,Compute
df_final_t = pd.DataFrame(columns=['Date','Type-1%','Type-2%','Type-3%','T+1%','T+2 to T+7%','T+8 to T+30%'])
inp_date = '2021-11-15'

df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between cast( '"""+inp_date+"""' as date)
                         and date_add(cast( '"""+inp_date+"""' as date),30))""")
df_busi_day=df_busi_day1.toPandas()
inp_list = df_busi_day['TradingDay']

for date in inp_list:
    date1 = date.strftime('%Y-%m-%d')
    out = dormancy_eda2(date1)
    list1 = list(out)
    list1.insert(0, date1)
    df_length = len(df_final_t)
    df_final_t.loc[df_length] = list1

print(df_final_t)

#Ran in 1.73 hours

# COMMAND ----------

# DBTITLE 1,Compute
#outname = 'df_final_t.csv'
#outdir = '/dbfs/FileStore/'
#df_final_t.to_csv(outdir+outname, index=False, encoding="utf-8")
df_final_t_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/df_final_t.csv')
display(df_final_t_csv)

# COMMAND ----------

# MAGIC %md
# MAGIC #####Insights
# MAGIC 1. Distribution of DTU as % in type1, type2, type3 has consistency
# MAGIC 2. To make the prediction actionable it is more important to identify if customer is part of a stable group in our customer-base than identifying his first trade date.
# MAGIC 3. We can say that type1 has more stable customers and type2 has both stable as well as unstable customers
# MAGIC 4. %Of customers decreases from Type1 to Type3
# MAGIC 5. We are lured to propose the following line of thought
# MAGIC    - Once customer becomes stable and he/she is trading at a stable frequency, and if he/she suddenly stops trading then its red flag (typically for Customer from Type1 )
# MAGIC 6. We understood from this analysis that modeling aggregate behavior of DTU customers is sufficient for creating a generalisable strategy
# MAGIC 7. We have to separately model type3  customers as we don't have much information about them.
# MAGIC 8. Distribution % from DTU increases from T+1 to T+30 as next trade bucket
# MAGIC 9. We now need to decide further which strata should be used for model and what should be the target of our modeling exercise, for that we need to explore each type in more detail

# COMMAND ----------

# MAGIC %md
# MAGIC ####Next Trade day 
# MAGIC - Analysis of  customers from Type1, Type2 and Type3 to understand trends of trade days count in each group for the next 30 days. 
# MAGIC 
# MAGIC 1. What is trend of trade days count in Type1,Type2,Type3 for next 30 days?
# MAGIC 2. Does customers in different types have consistent behaviour?
# MAGIC 3. Is there any relation between different day count buckets?
# MAGIC 4. Is there any defined behaviour based on different types and next trade day count bucket?
# MAGIC 5. Can we define homogenous strata/segments of customer for ML model?
# MAGIC 6. Can we decide on target variable and features to be used for ML model?
# MAGIC    - This is an evolving thing and for now we might go ahead with one such choice for the sake of completing this exploration
# MAGIC 
# MAGIC *So far we understood based on first trade we can categorize customers into 3 groups.*
# MAGIC *Now further to understand these group we decided to have different buckets like*
# MAGIC *Trade_day_count =1 (Only did first/one trade in 30 days) , Trade_day_count between 2 and 5, Trade_day_count between 5 and 10 and Trade_day_count greater than 10. This analysis will help us to understand how we can create different stratas based on first_trade and trade_day_count. Also how trade_day_count varies from type to type.* 
# MAGIC *We considered data from 15-Nov-2021 to 15-Dec-2021 for this analysis.*

# COMMAND ----------

# DBTITLE 1,Function defined to take date as a input and calculates trend of trade days count among different types
def dormancy_eda3(date1):
        
    #Out of total party code whose first trade was older than 3 months
    day0_type1_1_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) <= date_add(cast( '"""+date1+"""' as date),-90)""")

    day0_type1=day0_type1_1_1.toPandas()
    day0_type1_1_1.unpersist() 
    type1 = len(day0_type1.index)
    
    #Further analysis of type1 customers to see number of trade days for customers in next 30 dyas    
    day0_type1_count_1 = sqlContext.sql(f"""select party_code, count(distinct sauda_date) as trade_day from 
                                  as_ordercountdata where party_code in
                                (
                               select  party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) <= date_add(cast( '"""+date1+"""' as date),-90)


                               )  
                               and sauda_date between date_add(cast( '"""+date1+"""' as date),1) and date_add(cast( '"""+date1+"""' as date),30)
                               group by party_code""")
    
    day0_type1_count=day0_type1_count_1.toPandas()
    day0_type1_count_1.unpersist() 
    
    day0_type1_active_2_5 = day0_type1_count[day0_type1_count['trade_day'].between(2, 5)]
    type1_active_2_5 = len(day0_type1_active_2_5.index)
    type1_active_2_5_per = round(((type1_active_2_5/type1)*100),2)
    
    day0_type1_active_6_10 = day0_type1_count[day0_type1_count['trade_day'].between(6, 10)]
    type1_active_6_10 = len(day0_type1_active_6_10.index)
    type1_active_6_10_per = round(((type1_active_6_10/type1)*100),2)
    
    day0_type1_active_gt_10 = day0_type1_count[day0_type1_count['trade_day'] >10]
    type1_active_gt_10 = len(day0_type1_active_gt_10.index)
    type1_active_gt_10_per = round(((type1_active_gt_10/type1)*100),2)
    
    type1_active_1_per = round((100 - (type1_active_2_5_per+type1_active_6_10_per+type1_active_gt_10_per)),2)
 
   ##################################################################################################################
        
    #Out of total party code whose first trade within 3 months
    day0_type2_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) between date_add(cast( '"""+date1+"""' as date),-89)
                               and date_add(cast( '"""+date1+"""' as date),-1)""")
    day0_type2=day0_type2_1.toPandas()
    day0_type2_1.unpersist() 
    type2 = len(day0_type2.index)
    
    #Further analysis of type2 customers to see number of trade days for customers within next 30 days   
    day0_type2_count_1 = sqlContext.sql(f"""select party_code, count(distinct sauda_date) as trade_day from 
                                   as_ordercountdata where party_code in
                                (
                               select  party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) between date_add(cast( '"""+date1+"""' as date),-89)
                               and date_add(cast( '"""+date1+"""' as date),-1)

                               )  
                               and sauda_date between date_add(cast( '"""+date1+"""' as date),1) and date_add(cast( '"""+date1+"""' as date),30)
                               group by party_code""")
    
    
    day0_type2_count=day0_type2_count_1.toPandas()
    day0_type2_count_1.unpersist() 
    
    day0_type2_active_2_5 = day0_type2_count[day0_type2_count['trade_day'].between(2, 5)]
    type2_active_2_5 = len(day0_type2_active_2_5.index)
    type2_active_2_5_per = round(((type2_active_2_5/type2)*100),2)
    
    day0_type2_active_6_10 = day0_type2_count[day0_type2_count['trade_day'].between(6, 10)]
    type2_active_6_10 = len(day0_type2_active_6_10.index)
    type2_active_6_10_per = round(((type2_active_6_10/type2)*100),2)
    
    day0_type2_active_gt_10 = day0_type2_count[day0_type2_count['trade_day'] >10]
    type2_active_gt_10 = len(day0_type2_active_gt_10.index)
    type2_active_gt_10_per = round(((type2_active_gt_10/type2)*100),2)
    
    type2_active_1_per = round((100 - (type2_active_2_5_per+ type2_active_6_10_per + type2_active_gt_10_per)),2)
    
    ###################################################################################################################
    #Out of total party code whose first trade was on the same day
    day0_type3_1 = sqlContext.sql(f"""select distinct party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) = cast( '"""+date1+"""' as date)""")
     
    day0_type3=day0_type3_1.toPandas()
    day0_type3_1.unpersist()     
    type3 = len(day0_type3.index)
    
    #Further analysis of type3 customers to see number of trade days for customers within next 30 days
    day0_type3_count_1 = sqlContext.sql(f"""select party_code, count(distinct sauda_date) as trade_day from 
                                  as_ordercountdata where party_code in
                                (
                               select  party_code
                               from as_ordercountdata
                               where party_code in 
                               (select party_code from SN_ClientKYC where B2C='Y')
                               and party_code in (select distinct party_code
                               from as_ordercountdata where
                                sauda_date = cast( '"""+date1+"""' as date))
                               group by party_code having min(sauda_date) = cast( '"""+date1+"""' as date)


                               )  
                               and sauda_date between date_add(cast( '"""+date1+"""' as date),1) and date_add(cast( '"""+date1+"""' as date),30)
                               group by party_code""")
    
    
    day0_type3_count=day0_type3_count_1.toPandas()
    day0_type3_count_1.unpersist()  
    
    day0_type3_active_2_5 = day0_type3_count[day0_type3_count['trade_day'].between(2, 5)]
    type3_active_2_5 = len(day0_type3_active_2_5.index)
    type3_active_2_5_per = round(((type3_active_2_5/type3)*100),2)
    
    day0_type3_active_6_10 = day0_type3_count[day0_type3_count['trade_day'].between(6, 10)]
    type3_active_6_10 = len(day0_type3_active_6_10.index)
    type3_active_6_10_per = round(((type3_active_6_10/type3)*100),2)
    
    day0_type3_active_gt_10 = day0_type3_count[day0_type3_count['trade_day'] >10]
    type3_active_gt_10 = len(day0_type3_active_gt_10.index)
    type3_active_gt_10_per = round(((type3_active_gt_10/type3)*100),2)
    
    type3_active_1_per =round((100 - (type3_active_2_5_per+type3_active_6_10_per+type3_active_gt_10_per)),2)
     
    
    ####################################################################################################################
    return type1_active_1_per,type1_active_2_5_per,type1_active_6_10_per,type1_active_gt_10_per,type2_active_1_per,type2_active_2_5_per,type2_active_6_10_per,type2_active_gt_10_per,type3_active_1_per,type3_active_2_5_per,type3_active_6_10_per,type3_active_gt_10_per
    

# COMMAND ----------

# MAGIC %md
# MAGIC - Example of table: Distribution of different type based on number of active days
# MAGIC 
# MAGIC |Date|Active_days=1(Type1)|Active_days=2to5(Type1)|Active_days=5to10(Type1)|Active_days>10(Type1)|Active_days=1(Type2)|Active_days=2to5(Type2)|Active_days=5to10(Type2)|Active_days>10(Type2)|Active_days=1(Type3)|Active_days=2to5(Type3)|Active_days=5to10(Type3)|Active_days>10(Type3)|
# MAGIC |---|----|----|----|----|----|----|----|----|----|----|----|----|
# MAGIC |2021-11-15| | | | | | |

# COMMAND ----------

# DBTITLE 1,Compute
df_final = pd.DataFrame(columns=['Date','Active_days=1(Type1)','Active_days=2to5(Type1)','Active_days=5to10(Type1)','Active_days>10(Type1)','Active_days=1(Type2)','Active_days=2to5(Type2) ','Active_days=5to10(Type2)','Active_days>10(Type2)','Active_days=1(Type3)','Active_days=2to5(Type3)','Active_days=5to10(Type3)','Active_days>10(Type3)'])

inp_date1 = '2021-11-15'

#calculating business days between 2021-11-15 and 2021-12-15
df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between cast( '"""+inp_date1+"""' as date)
                         and date_add(cast( '"""+inp_date1+"""' as date),30))""")
df_busi_day=df_busi_day1.toPandas()
inp_list = df_busi_day['TradingDay']

for date in inp_list:
    date1 = date.strftime('%Y-%m-%d')
    out = dormancy_eda3(date1)                                #Calling function
    list1 = list(out)                                         #Converting output return from function to ist 
    list1.insert(0, date1)                                    #Inserting date at start of the list 
    df_length = len(df_final)
    df_final.loc[df_length] = list1                           #Inserting record at the end of dataframe

print(df_final)

#took 2.35 hours to run

# COMMAND ----------

# DBTITLE 1,Compute
#outname = 'df_final.csv'
#outdir = '/dbfs/FileStore/'
#df_final.to_csv(outdir+outname, index=False, encoding="utf-8")
df_final_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/df_final.csv')
display(df_final_csv)

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ######Insights
# MAGIC 
# MAGIC 1. We know that type1 customer-group is more stable and we find that a higher percentage fall in the bucket active_days > 10
# MAGIC 2. Values in each column have consistency hence we can say that customers have defined behavior
# MAGIC 3. Active days between 2-5 and 5-10 have largely same behavior throughout all types.
# MAGIC 4. Herein, we take one row from each type. We can say that numbers marked in bold are from stable customers. We can not make comment about Type2-column1 as it might be case that customer performed trade just one day before source date (e.g we are calculating for 15 nov and customer performed trade on 14 nov)
# MAGIC 
# MAGIC 
# MAGIC  |Type | trade_day_count=1 within T+30 | trade_day_count=2-5 within T+30| trade_day_count= 5-10 within T+30 |  Trade_day_count >10 within T+30|
# MAGIC  |-----|-------|------|--------|------|                  
# MAGIC  | Type3  | 48% |             25%                 |                   14%              |                  13%|
# MAGIC  | Type2  |            13%                  |             **23%**               |                   **22%**            |                  **42%**|
# MAGIC  | Type1  |             9%                  |             **18%**               |                   **21%**            |                  **52%** |
# MAGIC 
# MAGIC 
# MAGIC 5. Type 1 and Type2 exhibit the same behavior while Type 3 has reverse behavior compared to other types. This is because larger of this bucket is going to be dormant.
# MAGIC 6. Frequency and recency are important factors emerging in separating the classes in this model. 
# MAGIC 7. Based on this EDA we finalized that there will be 2 segments of customers one whose trade_day_count is less than 4(Unstable customers) and one whose trade_day_count is greater than 4 (stable customers).
# MAGIC 8. We can say by this EDA that there is defined behavior of the customer based on first trade and number of days the trades performed by the user. So if we provide Frequency,recency along with target variable inactive days business team can save customers from getting dormant.

# COMMAND ----------

# MAGIC %md
# MAGIC ###Reformulation
# MAGIC - We found that there is a great deal of heterogeneity in terms of inactive days of a customer even when they show a stable behavior
# MAGIC - In such a scenario, it is highly unlikely that a generic 30day or 60day inactivity prediction model trained will be able to separate actionable customers
# MAGIC 
# MAGIC Way Forward
# MAGIC - **Option 1: Get business help in picking a class of DTU customer for which 30day or 60day inactivity prediction is apriori assumed to be actionable and impactful**
# MAGIC - **Option 2: Get inactive days prediction and corresponding margin of error for each DTU customer and business subsequently decides that actionable class based on their historical recency and frequency metric**
# MAGIC    - **Option 2.5: Reduction in transaction, reason(s)? [Market Conditions]**
# MAGIC - **Option 3: Reformulate the problem as a specific churn-like event prediction, e.g app uninstall etc.**
# MAGIC - **Option 4: Treat the prediction as future segmentation logic and plan for deep-sell**
# MAGIC - **Option 5: Frame the project as daily user's RFM segmentation, DeS project** 

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ###Next-Trade-Day Model
# MAGIC 
# MAGIC - Targeting customers who have *trade_day_count* >=4.
# MAGIC 
# MAGIC ####Features used:
# MAGIC 1.trade_days_cnt
# MAGIC 
# MAGIC 2.total_trades
# MAGIC 
# MAGIC 3.total_order_count
# MAGIC 
# MAGIC 4.gender
# MAGIC 
# MAGIC 5.riskcategory
# MAGIC 
# MAGIC 6.incomedetails
# MAGIC 
# MAGIC 7.occupation
# MAGIC 
# MAGIC 8.age
# MAGIC 
# MAGIC 9.activeinsys
# MAGIC 
# MAGIC 10.city
# MAGIC 
# MAGIC 11.avg_investment
# MAGIC 
# MAGIC 12.mean_last3
# MAGIC 
# MAGIC 13.sta_dev
# MAGIC 
# MAGIC ####Target:
# MAGIC Inactive_days=Days difference between DTU date and next trade day

# COMMAND ----------

# DBTITLE 1,Function calculates training as well as testing data for model. This function used for day level data preparation
def data_prep_day(inp_date):
    inp_date1 = inp_date
    #Fetching data from database
    df1 = sqlContext.sql(f"""select a.Party_code,trade_days_cnt,total_trades,total_order_count,
                          gender,riskcategory,incomedetails,occupation,age,activeinsys,city,avg_investment,
                          nvl(next_trade,(date_add(cast( '"""+inp_date+"""' as date),65))) as next_trade 
                          from
                      
                         ((select Party_code, count(distinct sauda_date) as trade_days_cnt from 
                          as_ordercountdata where 
                          Party_code in 
                          (select distinct a.Party_code from as_ordercountdata as a 
                          ,SN_ClientKYC as b
                           where b.B2C = 'Y' and a.sauda_date = cast( '"""+inp_date+"""' as date) and a.Party_code = b.Party_Code)
                           and sauda_date between date_add(cast( '"""+inp_date+"""' as date),-30) and cast( '"""+inp_date+"""' as date)
                           group by Party_code having count(distinct sauda_date) >= 4 ) AS a
                       
                            left join
                       
                          (select Party_code, 
                          count(*) as total_trades,
                          sum(OC) as total_order_count,
                          AVG(T_O) AS avg_investment
                          from as_ordercountdata
                          where sauda_date between date_add(cast( '"""+inp_date+"""' as date),-30) and cast( '"""+inp_date+"""' as date)
                          group by party_code) AS  b
                       
                          on a.Party_code = b.Party_code
                       
                          left join
                       
                            (select distinct sn.Party_Code,
                           gender,
                           riskcategory,
                           incomedetails,
                           occupation,
                           DATEDIFF(birthdate, cast( '"""+inp_date+"""' as date))/365 AS age,
                           DATEDIFF(activefrom,cast( '"""+inp_date+"""' as date)) as activeinsys,
                           city
                           FROM SN_ClientKYC as sn 
                          where B2C='Y') AS c
                       
                           on  a.Party_code=c.Party_code
                       
                           left join 
                       
                          (select Party_code, min(sauda_date) as next_trade from 
                          as_ordercountdata where 
                          Party_code in 
                          (select distinct a.Party_code from as_ordercountdata as a ,SN_ClientKYC as b
                           where b.B2C = 'Y' and a.sauda_date = cast( '"""+inp_date+"""' as date) and a.Party_code = b.Party_Code)
                           and sauda_date between date_add(cast( '"""+inp_date+"""' as date),1) and  date_add(cast( '"""+inp_date+"""' as date),61)
                           group by Party_code having count(distinct sauda_date) >= 4) As d
                       
                           on a.Party_code = d.Party_code) """)
    df=df1.toPandas()
    df1.unpersist()
    #Calculating target variable
    df['next_trade']=pd.to_datetime(df['next_trade'])
    df['date']=inp_date
    df['date'] = pd.to_datetime(df['date'])
    dates1 = df['date'].values.astype('datetime64[D]')
    dates2 = df['next_trade'].values.astype('datetime64[D]')
    
    input = inp_date
    format = '%Y-%m-%d'
    datetime1 = datetime.datetime.strptime(input, format)
    inp_date=datetime1.date()
    
    #Calculate non business days:
    df["daterange"] = df.apply(lambda x: pd.date_range(x.date, x.next_trade), axis=1)
    df["daterange"] = df.apply(lambda x: set(x['daterange'].to_list()), axis=1)
    
    cal_day = pd.date_range(start=inp_date,end=inp_date+timedelta(days=60))
    list1 = cal_day.to_frame()[0].to_list()
    df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between cast( '"""+inp_date1+"""' as date)
                         and date_add(cast( '"""+inp_date1+"""' as date),60))""")
    df_busi_day=df_busi_day1.toPandas()
    list2 = df_busi_day['TradingDay']
    df['next_60_days_holi_cal'] = df.apply(lambda x: set(list1) ^ set(list2),axis=1)
    df["non_business_count"]=df.apply(lambda x: len(x.daterange & x.next_60_days_holi_cal),axis=1)
        
    #Doing this handling as we need to consider only business days   
    df['inactive_days'] = dates2 -dates1
    df['inactive_days'] = df['inactive_days'].dt.days -1 - df["non_business_count"]
    #Calculating default days for missing value    
    default=len(list1)-len(df['next_60_days_holi_cal'][0])
    #Missing value handling
    df['inactive_days'].fillna(60-default, inplace=True)
    #Calculating difference between last 3 purchases in last one month and stad dev & mean of day differences between purchases
    #Fetching last one month data for customer traded on input date 
    df_PC1 = sqlContext.sql(f""" select Party_code, sauda_date from as_ordercountdata
                           where sauda_date between date_add(cast( '"""+inp_date1+"""' as date),-30) and cast( '"""+inp_date1+"""' as date) and Party_code in
                           (select distinct Party_code from as_ordercountdata where sauda_date = cast( '"""+inp_date1+"""' as date))""")
    df_PC=df_PC1.toPandas()
    df_PC1.unpersist()
    df_PC = df_PC.sort_values(['Party_code','sauda_date'])
    df_PC = df_PC.drop_duplicates(subset=['Party_code','sauda_date'],keep='first')
    #shifting last 3 purchase dates
    df_PC['PrevSaudaDate'] = df_PC.groupby('Party_code')['sauda_date'].shift(1)
    df_PC['T2SaudaDate'] = df_PC.groupby('Party_code')['sauda_date'].shift(2)
    df_PC['T3SaudaDate'] = df_PC.groupby('Party_code')['sauda_date'].shift(3)
    df_PC = df_PC.drop_duplicates(subset=['Party_code'],keep='last')
    #Calculating past 30 days holiday calender   
    cal_day = pd.date_range(start=inp_date-timedelta(days=30),end=inp_date)
    list1 = cal_day.to_frame()[0].to_list()
    
    df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between date_add(cast( '"""+inp_date1+"""' as date),-30)  
                         and cast( '"""+inp_date1+"""' as date))""")
    df_busi_day=df_busi_day1.toPandas()
    df_busi_day1.unpersist()
    list2 = df_busi_day['TradingDay']
    past_30_days_holi_cal = set(list1) ^ set(list2)
    #Setting missing values   
    df_PC['PrevSaudaDate'].fillna(inp_date-timedelta(days=30), inplace=True)
    df_PC['T2SaudaDate_dummy'] = np.where(~df_PC['T2SaudaDate'].isnull(),0,30)
    df_PC['T2SaudaDate'].fillna(inp_date-timedelta(days=30), inplace=True)
    df_PC['T3SaudaDate_dummy'] = np.where(~df_PC['T3SaudaDate'].isnull(),0,60)
    df_PC['T3SaudaDate'].fillna(inp_date-timedelta(days=30), inplace=True)
    #Calculating datarange and removing days from holiday calender
    df_PC["daterange1"] = df_PC.apply(lambda x: pd.date_range(x.PrevSaudaDate, x.sauda_date), axis=1)
    df_PC["daterange1"] = df_PC.apply(lambda x: set(x['daterange1'].to_list()), axis=1)
    df_PC["non_business_count1"]=df_PC.apply(lambda x: len(x.daterange1 & past_30_days_holi_cal),axis=1)
    df_PC['DayDiff'] = (pd.to_datetime(df_PC['sauda_date']) - pd.to_datetime(df_PC['PrevSaudaDate'])).dt.days
    df_PC['DayDiff'] = df_PC['DayDiff']  - 1 - df_PC["non_business_count1"]
    df_PC["daterange2"] = df_PC.apply(lambda x: pd.date_range(x.T2SaudaDate, x.PrevSaudaDate), axis=1)
    df_PC["daterange2"] = df_PC.apply(lambda x: set(x['daterange2'].to_list()), axis=1)
    df_PC["non_business_count2"]=df_PC.apply(lambda x: len(x.daterange2 & past_30_days_holi_cal),axis=1)
    df_PC['DayDiff2'] = (pd.to_datetime(df_PC['PrevSaudaDate']) - pd.to_datetime(df_PC['T2SaudaDate'])).dt.days
    df_PC['DayDiff2'] = df_PC['DayDiff2'] -1 - df_PC["non_business_count2"]+df_PC['T2SaudaDate_dummy'] 
    df_PC["daterange3"] = df_PC.apply(lambda x: pd.date_range(x.T3SaudaDate, x.T2SaudaDate), axis=1)
    df_PC["daterange3"] = df_PC.apply(lambda x: set(x['daterange3'].to_list()), axis=1)
    df_PC["non_business_count3"]=df_PC.apply(lambda x: len(x.daterange3 & past_30_days_holi_cal),axis=1)
    df_PC['DayDiff3'] = (pd.to_datetime(df_PC['T2SaudaDate']) - pd.to_datetime(df_PC['T3SaudaDate'])).dt.days
    df_PC['DayDiff3'] = df_PC['DayDiff3'] -1 - df_PC["non_business_count3"]+df_PC['T3SaudaDate_dummy']
    df_PC['mean_last_3'] =  (df_PC['DayDiff'] + df_PC['DayDiff2']+ df_PC['DayDiff3'])/3
    df_PC['stddev_last_3'] = df_PC[['DayDiff', 'DayDiff2','DayDiff3']].std(axis=1) 
    df_PC = df_PC.drop(['PrevSaudaDate','T2SaudaDate','T3SaudaDate','T2SaudaDate_dummy','T3SaudaDate_dummy',
                        'daterange1','daterange3','non_business_count3','non_business_count1','daterange2',
                        'non_business_count2'],axis=1)
    df = pd.merge(df, df_PC, on='Party_code')
    #Converting city to tier
    tier_info1 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/Indian_Cities_by_Tier_I_and_II.csv')
    tier_info=tier_info1.toPandas()
    tier_info.dropna(subset=['Tier'], inplace=True)
    tier_mapper = tier_info[['City', 'Tier']].set_index('City').to_dict()['Tier']    
    def tier_mapping(city):
        try: return tier_mapper[city.capitalize()]
        except: return 'Tier-III'
    df['Tier'] = df['city'].map(tier_mapping)
    df = df.drop(['city'], axis = 1)
    #Missing values
    df['occupation'].fillna('other', inplace=True)
    df['riskcategory'].fillna('other', inplace=True)  
    #calculating age buckets
    def f(row):
        if row <=35:
            val = 1
        elif row > 35 and row<=45:
            val = 2
        else:
            val = 3
        return val
    df['new_age'] = df['age'].apply(f)
    #Calculating income buckets
    def f1(row):
        if row =='BELOW 1 LAC':
            val = 1
        elif row == '1-5 LAC':
            val = 2
        elif row == '5-10 LAC':
            val = 3
        elif row == '10-25 LAC':
            val = 4
        elif row == '>25 LAC':
            val = 5
        else:
            val = 0
        return val
    df['new_incomedetails'] = df['incomedetails'].apply(f1)

    df["gender"].replace({"f": "F"}, inplace=True)
    def f2(row):
        if row == 'F':
            val = 1
        else:
            val = 0
        return val
    df['gender_f'] = df['gender'].apply(f2)

    def f3(row):
        if row == 'HIGH':
            val = 2
        elif row == 'MEDIUM':
            val = 1
        else:
            val=0
        return val
    df['riskcategory_new'] = df['riskcategory'].apply(f3)
    df = df.drop(['gender','incomedetails','age','riskcategory','Party_code','next_trade','sauda_date','date','daterange'
                 ,'next_60_days_holi_cal','non_business_count'],axis=1)
    #onehot encoding
    print(df)
    X = df.copy()
    cat_attribs = ['occupation', 'Tier']

    # creating instance of one-hot-encoder
    enc = OneHotEncoder(handle_unknown='ignore')
    enc_df = pd.DataFrame(enc.fit_transform(X[cat_attribs]).toarray())
    #enc_df = pd.DataFrame(enc.transform(X[cat_attribs]).toarray())
    X = X.join(enc_df)
    X= X.drop(['Tier','occupation'],axis=1)
    Y= X['inactive_days']
    X_train,Y_train = X,Y
    X_train = X_train.drop(['inactive_days'], axis=1) 
    X_train.fillna(0, inplace=True)
    return (X_train,Y_train)

# COMMAND ----------

# DBTITLE 1,Function to train random-regressor model
def train_model(X_train,Y_train):
    forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)
    forest_reg.fit(X_train, Y_train)
    inactive_predictions = forest_reg.predict(X_train)
    forest_mse = mean_squared_error(Y_train, inactive_predictions) 
    forest_rmse = np.sqrt(forest_mse) 
    MAE=mean_absolute_error(Y_train, inactive_predictions)
    return (forest_rmse,MAE,forest_reg,inactive_predictions) 

# COMMAND ----------

# DBTITLE 1,Function performs testing of baseline and ML model 
#Test Function used for both randomeregressor and baseline model. model =None for baseline model
def test_model(X_test,Y_test, model=None):
    if model != None:
        print("randomregressor") 
        inactive_predictions = forest_reg.predict(X_test)
    else:
        inactive_predictions = X_test
    forest_mse = mean_squared_error(Y_test, inactive_predictions) 
    forest_rmse = np.sqrt(forest_mse) 
    MAE=mean_absolute_error(Y_test, inactive_predictions)
    return (forest_rmse,MAE,inactive_predictions) 

# COMMAND ----------

# DBTITLE 1,Bucketed Metric Impact function
#Function used for both ML model and baseline model
def bucket_cal(inactive_predictions,Y,a,b):
    inactive_predictions = pd.Series(inactive_predictions) 
    df = pd.concat([inactive_predictions, Y], axis=1)
    df.columns=['Prediction','True_value']
    df = df.dropna()
    df_bucket = df[(df['True_value'] >=a) & (df['True_value'] <=b)]
    forest_mse = mean_squared_error(df_bucket['Prediction'],df_bucket['True_value']) 
    forest_rmse = np.sqrt(forest_mse) 
    mae=mean_absolute_error(df_bucket['Prediction'],df_bucket['True_value'])
    return (forest_rmse,mae,df_bucket['True_value'])

# COMMAND ----------

# DBTITLE 1,Function calculates training as well as testing data for model. This function used for week level data preparation
def data_prep_week(inp_date):
    inp_date1 = inp_date
    #Fetching data from database
    df1 = sqlContext.sql(f"""select a.Party_code,trade_days_cnt,total_trades,total_order_count,
                          gender,riskcategory,incomedetails,occupation,age,activeinsys,city,avg_investment,
                          nvl(next_trade,(date_add(cast( '"""+inp_date+"""' as date),65))) as next_trade,inp_sauda_date 
                          from
                      
                         ((select Party_code, count(distinct sauda_date) as trade_days_cnt from 
                          as_ordercountdata where 
                          Party_code in 
                          (select distinct a.Party_code from as_ordercountdata as a 
                          ,SN_ClientKYC as b
                           where b.B2C = 'Y' and a.sauda_date between cast( '"""+inp_date+"""' as date) and date_add(cast( '"""+inp_date+"""' as date),4) 
                           and a.Party_code = b.Party_Code)
                           
                           and sauda_date between date_add(cast( '"""+inp_date+"""' as date),-30) and cast( '"""+inp_date+"""' as date)
                           group by Party_code having count(distinct sauda_date) >= 4 ) AS a
                       
                            left join
                       
                          (select Party_code, 
                          count(*) as total_trades,
                          sum(OC) as total_order_count,
                          AVG(T_O) AS avg_investment
                          from as_ordercountdata
                          where sauda_date between date_add(cast( '"""+inp_date+"""' as date),-30) and cast( '"""+inp_date+"""' as date)
                          group by party_code) AS  b
                       
                          on a.Party_code = b.Party_code
                       
                          left join
                       
                            (select distinct sn.Party_Code,
                           gender,
                           riskcategory,
                           incomedetails,
                           occupation,
                           DATEDIFF(birthdate, cast( '"""+inp_date+"""' as date))/365 AS age,
                           DATEDIFF(activefrom,cast( '"""+inp_date+"""' as date)) as activeinsys,
                           city
                           FROM SN_ClientKYC as sn 
                          where B2C='Y') AS c
                       
                           on  a.Party_code=c.Party_code
                       
                           left join 
                        
                          (select Party_code, min(sauda_date) as next_trade from 
                          as_ordercountdata where 
                          Party_code in 
                          (select distinct a.Party_code from as_ordercountdata as a ,SN_ClientKYC as b
                           where b.B2C = 'Y' and a.sauda_date between cast( '"""+inp_date+"""' as date) and date_add(cast( '"""+inp_date+"""' as date),4) 
                           and a.Party_code = b.Party_Code)
                           and sauda_date between date_add(cast( '"""+inp_date+"""' as date),5) and date_add(cast( '"""+inp_date+"""' as date),66)
                           group by Party_code having count(distinct sauda_date) >= 4) As d                       
                           
                           on a.Party_code = d.Party_code
                           
                           left join 
                       
                           (select Party_code,
                              max(sauda_date) as inp_sauda_date
                           from as_ordercountdata
                           where sauda_date between cast( '"""+inp_date+"""' as date) and date_add(cast( '"""+inp_date+"""' as date),4)
                           group by Party_code) As e
                       
                           on a.Party_code = e.Party_code) """)
    df=df1.toPandas()
    df1.unpersist()
    #Calculating target variable
    df['next_trade']=pd.to_datetime(df['next_trade'])
    df['inp_sauda_date']=pd.to_datetime(df['inp_sauda_date'])
    dates1 = df['inp_sauda_date'].values.astype('datetime64[D]')
    dates2 = df['next_trade'].values.astype('datetime64[D]')
     
    input = inp_date
    format = '%Y-%m-%d'
    datetime1 = datetime.datetime.strptime(input, format)
    inp_date=datetime1.date()  
    
    #Calculate non business days:
    df["daterange"] = df.apply(lambda x: pd.date_range(start= x.inp_sauda_date, end = x.next_trade, freq ='D'), axis=1)
    df["daterange"] = df.apply(lambda x: set(x['daterange'].to_list()), axis=1)
    
    cal_day = pd.date_range(start=inp_date,end=inp_date+timedelta(days=64))
    list1 = cal_day.to_frame()[0].to_list()
    df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between cast( '"""+inp_date1+"""' as date)
                         and date_add(cast( '"""+inp_date1+"""' as date),64))""")
    df_busi_day=df_busi_day1.toPandas()
    list2 = df_busi_day['TradingDay']
    df['next_60_days_holi_cal'] = df.apply(lambda x: set(list1) ^ set(list2),axis=1)
    df["non_business_count"]=df.apply(lambda x: len(x.daterange & x.next_60_days_holi_cal),axis=1)
        
    #Doing this handling as we need to consider only business days   
    df['inactive_days'] = dates2 -dates1
    df['inactive_days'] = df['inactive_days'].dt.days -1 - df["non_business_count"]
    
    #Calculating default days for missing value    
    default=len(list1)-len(df['next_60_days_holi_cal'][0])
    #Missing value handling
    df['inactive_days'].fillna(64-default, inplace=True)

    #Calculating difference between last 3 purchases and stad dev & mean of day differences between last 3 purchases    
    df_PC1 = sqlContext.sql(f""" select Party_code, sauda_date from as_ordercountdata
                           where sauda_date between date_add(cast( '"""+inp_date1+"""' as date),-30) and date_add(cast( '"""+inp_date1+"""' as date),4) 
                           and Party_code in
                           (select distinct Party_code from as_ordercountdata where sauda_date between cast( '"""+inp_date1+"""' as date) and 
                           date_add(cast( '"""+inp_date1+"""' as date),4))""")
    df_PC=df_PC1.toPandas()
    df_PC1.unpersist()    
    df_PC = df_PC.sort_values(['Party_code','sauda_date'])
    df_PC = df_PC.drop_duplicates(subset=['Party_code','sauda_date'],keep='first')
    #shifting last 3 purchase dates
    df_PC['PrevSaudaDate'] = df_PC.groupby('Party_code')['sauda_date'].shift(1)
    df_PC['T2SaudaDate'] = df_PC.groupby('Party_code')['sauda_date'].shift(2)
    df_PC['T3SaudaDate'] = df_PC.groupby('Party_code')['sauda_date'].shift(3)
    df_PC = df_PC.drop_duplicates(subset=['Party_code'],keep='last')
    #Calculating past 30 days holiday calender   
    cal_day = pd.date_range(start=inp_date-timedelta(days=30),end=inp_date+timedelta(days=4))
    list1 = cal_day.to_frame()[0].to_list()    
    df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between date_add(cast( '"""+inp_date1+"""' as date),-30)  
                         and date_add(cast( '"""+inp_date1+"""' as date),4))""")
    df_busi_day=df_busi_day1.toPandas()
    df_busi_day1.unpersist()    
    list2 = df_busi_day['TradingDay']
    past_30_days_holi_cal = set(list1) ^ set(list2)
    
    #Setting missing values   
    df_PC['PrevSaudaDate'].fillna(inp_date-timedelta(days=30), inplace=True)
    df_PC['T2SaudaDate_dummy'] = np.where(~df_PC['T2SaudaDate'].isnull(),0,30)
    df_PC['T2SaudaDate'].fillna(inp_date-timedelta(days=30), inplace=True)
    df_PC['T3SaudaDate_dummy'] = np.where(~df_PC['T3SaudaDate'].isnull(),0,60)
    df_PC['T3SaudaDate'].fillna(inp_date-timedelta(days=30), inplace=True)
    #Calculating datarange and removing days from holiday calender
    df_PC["daterange1"] = df_PC.apply(lambda x: pd.date_range(x.PrevSaudaDate, x.sauda_date), axis=1)
    df_PC["daterange1"] = df_PC.apply(lambda x: set(x['daterange1'].to_list()), axis=1)
    df_PC["non_business_count1"]=df_PC.apply(lambda x: len(x.daterange1 & past_30_days_holi_cal),axis=1)
    df_PC['DayDiff'] = (pd.to_datetime(df_PC['sauda_date']) - pd.to_datetime(df_PC['PrevSaudaDate'])).dt.days
    df_PC['DayDiff'] = df_PC['DayDiff']  - 1 - df_PC["non_business_count1"]
    df_PC["daterange2"] = df_PC.apply(lambda x: pd.date_range(x.T2SaudaDate, x.PrevSaudaDate), axis=1)
    df_PC["daterange2"] = df_PC.apply(lambda x: set(x['daterange2'].to_list()), axis=1)
    df_PC["non_business_count2"]=df_PC.apply(lambda x: len(x.daterange2 & past_30_days_holi_cal),axis=1)
    df_PC['DayDiff2'] = (pd.to_datetime(df_PC['PrevSaudaDate']) - pd.to_datetime(df_PC['T2SaudaDate'])).dt.days
    df_PC['DayDiff2'] = df_PC['DayDiff2'] -1 - df_PC["non_business_count2"]+df_PC['T2SaudaDate_dummy'] 
    df_PC["daterange3"] = df_PC.apply(lambda x: pd.date_range(x.T3SaudaDate, x.T2SaudaDate), axis=1)
    df_PC["daterange3"] = df_PC.apply(lambda x: set(x['daterange3'].to_list()), axis=1)
    df_PC["non_business_count3"]=df_PC.apply(lambda x: len(x.daterange3 & past_30_days_holi_cal),axis=1)
    df_PC['DayDiff3'] = (pd.to_datetime(df_PC['T2SaudaDate']) - pd.to_datetime(df_PC['T3SaudaDate'])).dt.days
    df_PC['DayDiff3'] = df_PC['DayDiff3'] -1 - df_PC["non_business_count3"]+df_PC['T3SaudaDate_dummy']
    df_PC['mean_last_3'] =  (df_PC['DayDiff'] + df_PC['DayDiff2']+ df_PC['DayDiff3'])/3
    df_PC['stddev_last_3'] = df_PC[['DayDiff', 'DayDiff2','DayDiff3']].std(axis=1) 
        
    df_PC = df_PC.drop(['PrevSaudaDate','T2SaudaDate','T3SaudaDate','T2SaudaDate_dummy','T3SaudaDate_dummy',
                        'daterange1','daterange3','non_business_count3','non_business_count1','daterange2',
                        'non_business_count2'],axis=1)     
    df = pd.merge(df, df_PC, on='Party_code')
    
    #Converting city to tier
    tier_info1 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/Indian_Cities_by_Tier_I_and_II.csv')
    tier_info=tier_info1.toPandas()
    tier_info.dropna(subset=['Tier'], inplace=True)
    tier_mapper = tier_info[['City', 'Tier']].set_index('City').to_dict()['Tier']
    def tier_mapping(city):
        try: return tier_mapper[city.capitalize()]
        except: return 'Tier-III'
    df['Tier'] = df['city'].map(tier_mapping)
    df = df.drop(['city'], axis = 1)
    
    #Missing values
    df['occupation'].fillna('other', inplace=True)
    df['riskcategory'].fillna('other', inplace=True)
    #Calculating age buckets
    def f(row):
        if row <=35:
            val = 1
        elif row > 35 and row<=45:
            val = 2
        else:
            val = 3
        return val
    df['new_age'] = df['age'].apply(f)
    #Calculating income buckets
    def f1(row):
        if row =='BELOW 1 LAC':
            val = 1
        elif row == '1-5 LAC':
            val = 2
        elif row == '5-10 LAC':
            val = 3
        elif row == '10-25 LAC':
            val = 4
        elif row == '>25 LAC':
            val = 5
        else:
            val = 0
        return val
    df['new_incomedetails'] = df['incomedetails'].apply(f1)

    df["gender"].replace({"f": "F"}, inplace=True)
    def f2(row):
        if row == 'F':
            val = 1
        else:
            val = 0
        return val
    df['gender_f'] = df['gender'].apply(f2)

    def f3(row):
        if row == 'HIGH':
            val = 2
        elif row == 'MEDIUM':
            val = 1
        else:
            val=0
        return val
    df['riskcategory_new'] = df['riskcategory'].apply(f3)
    
    df = df.drop(['gender','incomedetails','age','riskcategory','Party_code','next_trade','sauda_date','daterange'
                 ,'next_60_days_holi_cal','non_business_count','inp_sauda_date'],axis=1)
         
    #onehot encoding
    print(df)
    X = df.copy()
    cat_attribs = ['occupation', 'Tier']

    # creating instance of one-hot-encoder
    enc = OneHotEncoder(handle_unknown='ignore')
    enc_df = pd.DataFrame(enc.fit_transform(X[cat_attribs]).toarray())
    #enc_df = pd.DataFrame(enc.transform(X[cat_attribs]).toarray())
    X = X.join(enc_df)
    X= X.drop(['Tier','occupation'],axis=1)
    Y= X['inactive_days']
    X_train,Y_train = X,Y
    X_train = X_train.drop(['inactive_days'], axis=1) 
    X_train.fillna(0, inplace=True)     
    return (X_train,Y_train)


# COMMAND ----------

# MAGIC %md
# MAGIC ####Model metric 
# MAGIC 
# MAGIC **What is the appropriate model metric to evaluate the performance of dormancy model?**
# MAGIC 
# MAGIC 
# MAGIC 1. IS Random regressor good enough to showcase predictive power, giving good RMSE and MAE value? 
# MAGIC 2. Does a day-level training data suffice a model selection with good RMSE and MAE value? 
# MAGIC 3. Does RMSE and MAE when split in buckets of frequency class, represent buckets correctly? i.e as size of bucket decreases merics values should increases to increase weightage of under valued buckets. (For analysis purpose we have divided true values of test data into different buckets based on inactive days such as 0-2,3-5,6-10,11-39,greater than 40)?
# MAGIC 
# MAGIC 
# MAGIC **Background**
# MAGIC 
# MAGIC 1. By EDA so far we understood that as number of inactive days increases their customer count will decrease. 
# MAGIC 2. For instance we will have more number of customers with 0 inactive days compare to customers with 30 inactive days. 
# MAGIC 3. We need to divide inactive days into buckets like 0-2, 3-5, 6-10,11-39 and greater than 40 to perform analysis(We decided buckets using KDE plot). 
# MAGIC 4. Our metrics value should increase as a number of inactive days bucket increases to give them higher weightage as their number is decreasing. 
# MAGIC 5. As our problem is regression problem we will testify 2 metrics i.e RMSE (Root Mean Squared Error) and MAE (Mean Absolute error). 
# MAGIC 6. We took 1st DEC as a DTU date and fetched their trade level data for last one month. We calculated target variable by subtracting days between T day and next trade  after T day.  

# COMMAND ----------

# MAGIC %md
# MAGIC **Calling functions for calculating values for RMSE, MAE using day as a input and randomregressor as a model**
# MAGIC 
# MAGIC Data preparation for train by taking day as a input

# COMMAND ----------

# DBTITLE 1,Compute
inp_date = '2021-12-01'
print(inp_date)

#Checking if input is valid business day
df_check = sqlContext.sql(f"""select TradingDay from SN_TradingDays where TradingDay=cast( '"""+inp_date+"""' as date)""")
if (df_check.rdd.isEmpty()):
    print("Input date is not a valid business date")
    sys.exit()
else:
    print("Call function")
    df_check.unpersist()
    X_train,Y_train = data_prep_day(inp_date)

#These changes are for storing required columns so that we can use them in baseline model analysis
baseline_model_df = pd.DataFrame(columns = ['Inp_date','Actual_Value','Mean']) 
Mean = X_train['mean_last_3']
baseline_model_df1=pd.concat([Y_train,Mean] ,axis=1)
baseline_model_df1.rename(columns = {'Y_train':'Actual_Value'}, inplace = True)
baseline_model_df1['Inp_date']=inp_date
first_column = baseline_model_df1.pop('Inp_date')
baseline_model_df1.insert(0, 'Inp_date', first_column)
baseline_model_df=baseline_model_df.append(baseline_model_df1, ignore_index=True)
#######################################################################################

# COMMAND ----------

# MAGIC %md
# MAGIC KDE plot for deciding inactive days buckets

# COMMAND ----------

sns.kdeplot(Y_train);

# COMMAND ----------

# MAGIC %md
# MAGIC Model building

# COMMAND ----------

# DBTITLE 1,Compute
print("Calling training model")
rmse_train,MAE,forest_reg,inactive_predictions = train_model(X_train,Y_train)
#Bucketwise calculation for train
list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
forest_rmse_list =[]
mae_list = []
data_count_list = []
for i in list1:
    forest_rmse,mae,True_value = bucket_cal(inactive_predictions,Y_train,i[0],i[1])
    forest_rmse_list.append(forest_rmse)
    mae_list.append(mae)
    data_count_list.append(True_value.count())

print("Overall and Bucketwise training performance for day as input and randomregressor as a model")
train_result_df = pd.DataFrame(list(zip(list1,data_count_list,forest_rmse_list,mae_list)),columns =['Buckets','Total_records','RMSE','MAE'])
train_result_df['inp_date']=inp_date
train_result_df.index= list(range(1, len(train_result_df) +1))
train_result_df.loc[0] = ['Overall',Y_train.count(),rmse_train,MAE,inp_date]
train_result_df = train_result_df.sort_index()
first_column = train_result_df.pop('inp_date')
train_result_df.insert(0, 'inp_date', first_column)
print(train_result_df)

# COMMAND ----------

# DBTITLE 1,Compute
#outname = 'train_result_df.csv'
#outdir = '/dbfs/FileStore/'
#train_result_df.to_csv(outdir+outname, index=False, encoding="utf-8")
train_result_df_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/train_result_df.csv')
display(train_result_df_csv)
df_train_rrday=train_result_df_csv.toPandas()
inp_display5 = ['2021-12-01'] * 6
df_train_rrday['Input_date'] = inp_display5
#Total records in dataset as well as records in each bucket for training data
sns.catplot(x="Input_date", y="Total_records", hue="Buckets", kind="bar", data=df_train_rrday,dodge=True,legend=True).set(title="Train Input_date vs Total_records for input=day and model=Randomregressor").set(ylim=(0, 250000));
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_date", y="RMSE", hue="Buckets", kind="bar", data=df_train_rrday,dodge=True,legend=True).set(title="Train Input_date vs RMSE for input=day and model=Randomregressor");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_date", y="MAE", hue="Buckets", kind="bar", data=df_train_rrday,dodge=True,legend=True).set(title="Train Input_date vs MAE for input=day and model=Randomregressor");

# COMMAND ----------

# MAGIC %md
# MAGIC ###Instrument type analysis
# MAGIC 
# MAGIC - As we can see from above visualization total number of customers in bucket 0-2 are more, hence to understand if their Instrument type is intra day we did following analysis.
# MAGIC 
# MAGIC - And we found that this whole subset is not from intra day rather it distributed across different instrument types.  

# COMMAND ----------

# DBTITLE 1,Overall INST_TYPE distribution for DTU
inp_date_list = ['2021-12-01','2021-12-02','2021-12-03','2021-12-08']
df_day_inst_type = pd.DataFrame(columns = ['sauda_date', 'INST_TYPE', 'Total'])
for inp_date in inp_date_list:
    df_inst_type = sqlContext.sql(f"""select sauda_date,INST_TYPE, count(*) as Total from as_ordercountdata where sauda_date=cast( '"""+inp_date+"""' as date) group by      sauda_date,INST_TYPE""")
    df_inst_type1 = df_inst_type.toPandas()
    df_day_inst_type = df_day_inst_type.append(df_inst_type1,ignore_index=True) 
print (df_day_inst_type)   
#Visualization
sns.catplot(x="sauda_date", y="Total", hue="INST_TYPE", kind="bar", data=df_day_inst_type,dodge=True,legend=True).set(title="Instrument Type distribution for DTU");


# COMMAND ----------

# DBTITLE 1,Bucketwise INST_TYPE distribution for DTU
inp_date_list = ['2021-12-01','2021-12-02','2021-12-03','2021-12-08']

df_master_bucket_inst = pd.DataFrame(columns = ['Inactive_days_Bucket', 'INST_TYPE', 'Total'])
for inp_date in inp_date_list:
    inp_date1 = inp_date
    df_bucket_inst_type = sqlContext.sql(f"""select a.Party_code, next_trade,INST_Type from
                                      ((select Party_code, min(sauda_date) as next_trade from 
                                       as_ordercountdata where Party_code in 
                                       (select distinct a.Party_code from as_ordercountdata as a 
                                       ,SN_ClientKYC as b
                                       where b.B2C = 'Y' and a.sauda_date = cast( '"""+inp_date+"""' as date) and a.Party_code = b.Party_Code)
                                       and sauda_date between date_add(cast( '"""+inp_date+"""' as date),1) and date_add(cast( '"""+inp_date+"""' as date),61)
                                       group by Party_code having count(distinct sauda_date) >= 4) As a
                                      
                                      left join 
                                      
                                      (select Party_code,INST_TYPE from as_ordercountdata 
                                      where sauda_date = cast( '"""+inp_date+"""' as date)) b
                                      on a.Party_code = b.Party_code)""")
                                      
    df=df_bucket_inst_type.toPandas()
    df_bucket_inst_type.unpersist()
    #Calculating target variable
    df['next_trade']=pd.to_datetime(df['next_trade'])
    df['date']=inp_date
    df['date'] = pd.to_datetime(df['date'])
    dates1 = df['date'].values.astype('datetime64[D]')
    dates2 = df['next_trade'].values.astype('datetime64[D]')

    input = inp_date
    format = '%Y-%m-%d'
    datetime1 = datetime.datetime.strptime(input, format)
    inp_date=datetime1.date()
    
    #Calculate non business days:
    df["daterange"] = df.apply(lambda x: pd.date_range(x.date, x.next_trade), axis=1)
    df["daterange"] = df.apply(lambda x: set(x['daterange'].to_list()), axis=1)
    
    cal_day = pd.date_range(start=inp_date,end=inp_date+timedelta(days=60))
    list1 = cal_day.to_frame()[0].to_list()
    df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between cast( '"""+inp_date1+"""' as date)
                                        and date_add(cast( '"""+inp_date1+"""' as date),60))""")
    df_busi_day=df_busi_day1.toPandas()
    list2 = df_busi_day['TradingDay']
    df['next_60_days_holi_cal'] = df.apply(lambda x: set(list1) ^ set(list2),axis=1)
    df["non_business_count"]=df.apply(lambda x: len(x.daterange & x.next_60_days_holi_cal),axis=1)
        
    #Doing this handling as we need to consider only business days   
    df['inactive_days'] = dates2 -dates1
    df['inactive_days'] = df['inactive_days'].dt.days -1 - df["non_business_count"]
    #Calculating default days for missing value    
    default=len(list1)-len(df['next_60_days_holi_cal'][0])
    #Missing value handling
    df['inactive_days'].fillna(60-default, inplace=True)
    
    #Calculating bucket for inactive days
    def f_bucket(inactive_days):
        if inactive_days >= 0 and inactive_days  <=2 :
            val = '0-2'
        elif inactive_days >= 3 and inactive_days  <=5 :
            val = '3-5'
        elif inactive_days >= 6 and inactive_days  <=10 :
            val = '6-10'
        elif inactive_days >= 11 and inactive_days  <=39 :
            val = '11-39'
        else:
            val = '40-50'
        return val
    df['Inactive_days_Bucket'] = df['inactive_days'].apply(f_bucket)
    df = df.drop(['inactive_days','Party_code','next_trade','daterange','next_60_days_holi_cal','non_business_count'], axis = 1)
    sparkDF=spark.createDataFrame(df) 
    sparkDF.createOrReplaceTempView("bucket_instrument")
    sparkDF.unpersist()
    df_inst_bucket = sqlContext.sql(f"""select Inactive_days_Bucket,INST_TYPE, count(*) as Total from bucket_instrument group by Inactive_days_Bucket,INST_TYPE""")
    #For saving result to csv
    df_inst_bucket1 = df_inst_bucket.toPandas()
    df_inst_bucket1['Input_date'] = inp_date
    df_master_bucket_inst = df_master_bucket_inst.append(df_inst_bucket1,ignore_index=True) 

print(df_master_bucket_inst)


# COMMAND ----------

# DBTITLE 1,Compute
#outname = 'df_master_bucket_inst.csv'
#outdir = '/dbfs/FileStore/'
#df_master_bucket_inst.to_csv(outdir+outname, index=False, encoding="utf-8")
df_master_bucket_inst_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/df_master_bucket_inst.csv')
display(df_master_bucket_inst_csv)
df_master_bucket_inst_pd = df_master_bucket_inst_csv.toPandas()
inp_date_list = ['2021-12-01','2021-12-02','2021-12-03','2021-12-08']
for inp_date in inp_date_list:
    out_df = df_master_bucket_inst_pd[df_master_bucket_inst_pd['Input_date'] == inp_date]
    print(inp_date)
    sns.catplot(x="Inactive_days_Bucket", y="Total", hue="INST_TYPE", kind="bar", data=out_df,dodge=True,legend=True).set(title="Bucketwise Instrument Type distribution for DTU");



# COMMAND ----------

# MAGIC %md
# MAGIC Data preparation for randomregressor test and model testing (Testing model on 3 different dates)

# COMMAND ----------

# DBTITLE 1,Compute
inp_date_list = ['2021-12-02','2021-12-03','2021-12-08']
master_test_result_df = pd.DataFrame(columns = ['Buckets', 'Total_records', 'RMSE','MAE','inp_date'])

for inp_date in inp_date_list:
    print(inp_date)
    #Checking if date is valid business day
    df_check = sqlContext.sql(f"""select TradingDay from SN_TradingDays where TradingDay=cast( '"""+inp_date+"""' as date)""")
    if (df_check.rdd.isEmpty()):
        print("Input date is not a valid business date")
        sys.exit()
    else:
        df_check.unpersist()
        X_test,Y_test = data_prep_day(inp_date)
        
        #These changes are for storing required columns so that we can use them in baseline model analysis
        baseline_model_df1 = Y_test,X_test['mean_last_3']
        Mean = X_test['mean_last_3']
        baseline_model_df1=pd.concat([Y_test,Mean] ,axis=1)
        baseline_model_df1.rename(columns = {'Y_test':'Actual_Value'}, inplace = True)
        baseline_model_df1['Inp_date']=inp_date
        first_column = baseline_model_df1.pop('Inp_date')
        baseline_model_df1.insert(0, 'Inp_date', first_column)
        baseline_model_df=baseline_model_df.append(baseline_model_df1, ignore_index=True)
        ####################################################################################################
    #Calling test model    
    rmse_test,mae_test,inactive_predictions = test_model(X_test, Y_test,forest_reg)

    #bucketwise calculation for test
    list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
    rmse_bucket_list =[]
    mae_bucket_list = []
    bucket_count_list = []
    for i in list1:
        forest_rmse,mae,True_value = bucket_cal(inactive_predictions,Y_test,i[0],i[1])
        rmse_bucket_list.append(forest_rmse)
        mae_bucket_list.append(mae)
        bucket_count_list.append(True_value.count())
    test_result_df = pd.DataFrame(list(zip(list1,bucket_count_list,rmse_bucket_list,mae_bucket_list)),columns =['Buckets','Total_records','RMSE','MAE'])
    test_result_df['inp_date']=inp_date
    test_result_df.index= list(range(1, len(test_result_df) +1))
    test_result_df.loc[0] = ['Overall',Y_test.count(),rmse_test,mae_test,inp_date]
    test_result_df = test_result_df.sort_index()
    master_test_result_df=master_test_result_df.append(test_result_df, ignore_index=True)

first_column = master_test_result_df.pop('inp_date')
master_test_result_df.insert(0, 'inp_date', first_column)

baseline_model_df=baseline_model_df.drop(['Actual_Value','Mean'],axis=1)    #Final df for baseline model

print("Overall and bucket wise performance of Test data by using randomregressor as model and day as input")    
print(master_test_result_df)


# COMMAND ----------

# DBTITLE 1,Compute
outname = 'master_test_result_df.csv'
outdir = '/dbfs/FileStore/'
master_test_result_df.to_csv(outdir+outname, index=False, encoding="utf-8")
master_test_result_df_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/master_test_result_df.csv')
display(master_test_result_df_csv)
master_test_result_df_viz = master_test_result_df_csv.toPandas()
inp_display_11= ['2021-12-02']
inp_display_11 = inp_display_11 * 6
inp_display_12= ['2021-12-03']
inp_display_12 = inp_display_12 * 6
inp_display_13= ['2021-12-08']
inp_display_13 = inp_display_13 * 6
inp_date_display =  inp_display_11 + inp_display_12 + inp_display_13
master_test_result_df_viz['Input_date'] = inp_date_display
#Total records in dataset as well as records in each bucket for training data 
sns.catplot(x="Input_date", y="Total_records", hue="Buckets", kind="bar", data=master_test_result_df_viz,dodge=True,legend=True).set(title="Testing Input_date vs Total_records for input=day and model=Randomregressor").set(ylim=(0, 250000));
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_date", y="RMSE", hue="Buckets", kind="bar", data=master_test_result_df_viz,dodge=True,legend=True).set(title="Testing Input_date vs RMSE for input=day and model=Randomregressor");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_date", y="MAE", hue="Buckets", kind="bar", data=master_test_result_df_viz,dodge=True,legend=True).set(title="Testing Input_date vs MAE for input=day and model=Randomregressor");

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC #####Insights 
# MAGIC 
# MAGIC 1. RMSE and MAE both are increasing as bucket size increases 
# MAGIC 2. RMSE numbers are high compare to MAE so we can choose RMSE as our metric
# MAGIC 3. Customers with 0 inactive days are dominating the population. So we can separate them  out and perform some other handling e.g maintaining their separate table

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ####Baseline Model
# MAGIC 
# MAGIC **Analyze if we should use ML model or baseline model with mean of inactive days from last 3 trades in past month**
# MAGIC 
# MAGIC 
# MAGIC 1. Is baseline model giving good RMSE/MAE value compare to ML model?
# MAGIC 2. In baseline model is RMSE/MAE increases as bucket size increases?
# MAGIC 
# MAGIC 
# MAGIC **Background:**
# MAGIC - We wanted to find if we need any ML model for prediction or simple rule based model will serve the purpose. 
# MAGIC - For that we created 2 models :
# MAGIC     1. Random regressor: It will take one date as training data and another date as testing data and will perform the modeling.
# MAGIC     2. Rule based modeling: We calculated mean of inactive days between last 3 trades in last month. Calculated RMSE and MSE by considering mean values as a predictor against actual values. 

# COMMAND ----------

# MAGIC %md
# MAGIC Running Baseline model on 4 dates

# COMMAND ----------

# DBTITLE 1,Compute
list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]       #buckets based on inactive days
inp_date_list = ['2021-12-01','2021-12-02','2021-12-03','2021-12-08']
master_test_result_df1 = pd.DataFrame(columns = ['Buckets', 'Total_records', 'RMSE','MAE','inp_date'])

for inp_date in inp_date_list:
    print(inp_date)
    df_inp = baseline_model_df[(baseline_model_df['Inp_date'] == inp_date)]
    rmse_test,mae_test,inactive_predictions = test_model(df_inp['mean_last_3'], df_inp['inactive_days'])     #calling test model function
    forest_rmse_list =[]
    mae_list = []
    data_count_list = []
    
    #Bucket level calculation
    for i in list1:
        forest_rmse,mae,True_value = bucket_cal(inactive_predictions,df_inp['inactive_days'],i[0],i[1])
        forest_rmse_list.append(forest_rmse)
        mae_list.append(mae)
        data_count_list.append(True_value.count())

    test_result_df = pd.DataFrame(list(zip(list1,data_count_list,forest_rmse_list,mae_list)),columns =['Buckets','Total_records','RMSE','MAE'])
    test_result_df['inp_date']=inp_date
    test_result_df.index= list(range(1, len(test_result_df) +1))      #For inserting overall data as a first row
    test_result_df.loc[0] = ['Overall',df_inp['mean_last_3'].count(),rmse_test,mae_test,inp_date]
    test_result_df = test_result_df.sort_index()                           
    master_test_result_df1=master_test_result_df1.append(test_result_df, ignore_index=True)
    
first_column = master_test_result_df1.pop('inp_date')                   #For adding input date as a first column
master_test_result_df1.insert(0, 'inp_date', first_column)
    
print("Overall and bucket wise performance of baseline model by taking day as a input")    
print(master_test_result_df1)

# COMMAND ----------

# DBTITLE 1,Compute
outname = 'master_test_result_df1.csv'
outdir = '/dbfs/FileStore/'
master_test_result_df1.to_csv(outdir+outname, index=False, encoding="utf-8")
master_test_result_df1_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/master_test_result_df1.csv')
display(master_test_result_df1_csv)
master_test_result_df1_csv_viz = master_test_result_df1_csv.toPandas()
inp_display_10= ['2021-12-01']
inp_display_10 = inp_display_10 * 6
inp_display_11= ['2021-12-02']
inp_display_11 = inp_display_11 * 6
inp_display_12= ['2021-12-03']
inp_display_12 = inp_display_12 * 6
inp_display_13= ['2021-12-08']
inp_display_13 = inp_display_13 * 6
inp_date_display = inp_display_10 + inp_display_11 + inp_display_12 + inp_display_13
master_test_result_df1_csv_viz['Input_date'] = inp_date_display
#Total records in dataset as well as records in each bucket for training data 
sns.catplot(x="Input_date", y="Total_records", hue="Buckets", kind="bar", data=master_test_result_df1_csv_viz,dodge=True,legend=True).set(title="Input_date vs Total_records for input=day and model=Baseline").set(ylim=(0, 250000));
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_date", y="RMSE", hue="Buckets", kind="bar", data=master_test_result_df1_csv_viz,dodge=True,legend=True).set(title="Input_date vs RMSE for input=day and model=Baseline");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_date", y="MAE", hue="Buckets", kind="bar", data=master_test_result_df1_csv_viz,dodge=True,legend=True).set(title="Input_date vs MAE for input=day and model=Baseline");

# COMMAND ----------

# MAGIC %md
# MAGIC #####Insights
# MAGIC 1. If we check bucketwise metrics values we can see that ML model is performing better compare to rule based model. Hence we can consider ML model for our use case. 
# MAGIC 2. In baseline model also RMSE/MAE increasing as bucket size is increasing

# COMMAND ----------

# MAGIC %md
# MAGIC ####Training Window 
# MAGIC 
# MAGIC **Analyze granularity of data whether we should take  train/test as single day or we should consider DTU of whole week for train/test data.**
# MAGIC 
# MAGIC 
# MAGIC 1. Is day level Random regressor is performing better compare to week level random regressor?
# MAGIC 2. Is day level baseline model is performing better compare to week level random regressor?
# MAGIC 3. What granularity we should consider for building our model?
# MAGIC 
# MAGIC 
# MAGIC **Background:**
# MAGIC - We wanted to identify at which granular level we should consider data to train and test. For that purpose we considered 2 granular level Day(We will consider DTU for one day) and Week(We will consider DTU for one week). 
# MAGIC - While calculating data for week level for next 60 days we considered, Last day of week + 60 and for last 30 days data considered, start day of week - 30 days

# COMMAND ----------

# MAGIC %md
# MAGIC Data preparation for randomeregressor model train by taking week as a input

# COMMAND ----------

# DBTITLE 1,Compute
inp_date = '2021-12-06'   #Input is first day of week 2021-12-06 to 2021-12-10
print(inp_date)
#Checking if input date is valid date
df_check = sqlContext.sql(f"""select TradingDay from SN_TradingDays where TradingDay=cast( '"""+inp_date+"""' as date)""")
if (df_check.rdd.isEmpty()):
    print("Input date is not a valid business date")
    sys.exit()
else:
    print("Call function")
    df_check.unpersist()
    X_train,Y_train = data_prep_week(inp_date)

#These changes are for storing required columns so that we can use them in baseline model analysis
baseline_model_df = pd.DataFrame(columns = ['Inp_date','Actual_Value','Mean']) 
Mean = X_train['mean_last_3']
baseline_model_df1=pd.concat([Y_train,Mean] ,axis=1)
baseline_model_df1.rename(columns = {'Y_train':'Actual_Value'}, inplace = True)
baseline_model_df1['Inp_date']=inp_date
first_column = baseline_model_df1.pop('Inp_date')
baseline_model_df1.insert(0, 'Inp_date', first_column)
baseline_model_df=baseline_model_df.append(baseline_model_df1, ignore_index=True)
#######################################################################################

# COMMAND ----------

# MAGIC %md
# MAGIC Randomregressor Model building

# COMMAND ----------

# DBTITLE 1,Compute
print("Calling training model")
rmse_train,MAE,forest_reg,inactive_predictions = train_model(X_train,Y_train)
#Bucketwise calculation for train
list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
forest_rmse_list =[]
mae_list = []
data_count_list = []

#Calculating bucketwise values
for i in list1:
    forest_rmse,mae,True_value = bucket_cal(inactive_predictions,Y_train,i[0],i[1])
    forest_rmse_list.append(forest_rmse)
    mae_list.append(mae)
    data_count_list.append(True_value.count())
print("Overall and Bucketwise training performance")
train_result_df1 = pd.DataFrame(list(zip(list1,data_count_list,forest_rmse_list,mae_list)),columns =['Buckets','Total_records','RMSE','MAE'])
train_result_df1['inp_date']=inp_date
train_result_df1.index= list(range(1, len(train_result_df1) +1))
train_result_df1.loc[0] = ['Overall',Y_train.count(),rmse_train,MAE,inp_date]
train_result_df1 = train_result_df1.sort_index()
first_column = train_result_df1.pop('inp_date')
train_result_df1.insert(0, 'inp_date', first_column)
print(train_result_df1)

# COMMAND ----------

# DBTITLE 1,Compute
outname = 'train_result_df1.csv'
outdir = '/dbfs/FileStore/'
train_result_df1.to_csv(outdir+outname, index=False, encoding="utf-8")
train_result_df1_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/train_result_df1.csv')
display(train_result_df1_csv)
train_result_df1_csv_viz = train_result_df1_csv.toPandas()
inp_display4 = ['2021-12-01'] * 6
train_result_df1_csv_viz['Input_week'] = inp_display4
#Total records in dataset as well as records in each bucket for training data 
sns.catplot(x="Input_week", y="Total_records", hue="Buckets", kind="bar", data=train_result_df1_csv_viz,dodge=True,legend=True).set(title="Training Input_week vs Total_records for input=week and model=RandomRegressor");
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_week", y="RMSE", hue="Buckets", kind="bar", data=train_result_df1_csv_viz,dodge=True,legend=True).set(title="Training Input_week vs RMSE for input=week and model=RandomRegressor");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_week", y="MAE", hue="Buckets", kind="bar", data=train_result_df1_csv_viz,dodge=True,legend=True).set(title="Training Input_week vs MAE for input=week and model=RandomRegressor");

# COMMAND ----------

# MAGIC %md
# MAGIC Data preparation for randomeRegressor test and model testing (Testing model on 2 different weeks)

# COMMAND ----------

# DBTITLE 1,Compute
inp_date_list = ['2021-12-13','2022-01-10']                     #Date is start day of week
master_test_result_df2 = pd.DataFrame(columns = ['Buckets', 'Total_records', 'RMSE','MAE','inp_date'])

for inp_date in inp_date_list:
    print(inp_date)
    df_check = sqlContext.sql(f"""select TradingDay from SN_TradingDays where TradingDay=cast( '"""+inp_date+"""' as date)""")
    if (df_check.rdd.isEmpty()):
        print("Input date is not a valid business date")
        sys.exit()
    else:
        df_check.unpersist()
        X_test,Y_test = data_prep_day(inp_date)
        
        #These changes are for storing required columns so that we can use them in baseline model analysis
        baseline_model_df1 = Y_test,X_test['mean_last_3']
        Mean = X_test['mean_last_3']
        baseline_model_df1=pd.concat([Y_test,Mean] ,axis=1)
        baseline_model_df1['Inp_date']=inp_date
        first_column = baseline_model_df1.pop('Inp_date')
        baseline_model_df1.insert(0, 'Inp_date', first_column)
        baseline_model_df=baseline_model_df.append(baseline_model_df1, ignore_index=True)
        ####################################################################################################
    rmse_test,mae_test,inactive_predictions = test_model(X_test, Y_test,forest_reg)             #Testing model

    #bucketwise calculation for test
    list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
    rmse_bucket_list =[]
    mae_bucket_list = []
    bucket_count_list = []

    for i in list1:
        forest_rmse,mae,True_value = bucket_cal(inactive_predictions,Y_test,i[0],i[1])
        rmse_bucket_list.append(forest_rmse)
        mae_bucket_list.append(mae)
        bucket_count_list.append(True_value.count())
    test_result_df = pd.DataFrame(list(zip(list1,bucket_count_list,rmse_bucket_list,mae_bucket_list)),columns =['Buckets','Total_records','RMSE','MAE'])
    test_result_df['inp_date']=inp_date
    test_result_df.index= list(range(1, len(test_result_df) +1))
    test_result_df.loc[0] = ['Overall',Y_test.count(),rmse_test,mae_test,inp_date]
    test_result_df = test_result_df.sort_index()
    master_test_result_df2=master_test_result_df2.append(test_result_df, ignore_index=True)

first_column = master_test_result_df2.pop('inp_date')
master_test_result_df2.insert(0, 'inp_date', first_column)

#baseline_model_df=baseline_model_df.drop(['Actual_Value','Mean'],axis=1)    #Final df for baseline model
print("Overall and bucket wise performance of randomregressor model by taking week as a input")    
print(master_test_result_df2)


# COMMAND ----------

# DBTITLE 1,Compute
outname = 'master_test_result_df2.csv'
outdir = '/dbfs/FileStore/'
master_test_result_df2.to_csv(outdir+outname, index=False, encoding="utf-8")
master_test_result_df2_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/master_test_result_df2.csv')
display(master_test_result_df2_csv)
master_test_result_df2_viz = master_test_result_df2_csv.toPandas()
inp_display_2= ['2021-12-13']
inp_display_2 = inp_display_2 * 6
inp_display_3= ['2022-01-10']
inp_display_3 = inp_display_3 * 6
inp_display = inp_display_2 + inp_display_3
master_test_result_df2_viz['Input_week'] = inp_display
#Total records in dataset as well as records in each bucket for training data 
sns.catplot(x="Input_week", y="Total_records", hue="Buckets", kind="bar", data=master_test_result_df2_viz,dodge=True,legend=True).set(title="Input_week vs Total_records for input=week and model=RandomRegressor").set(ylim=(0, 250000));
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_week", y="RMSE", hue="Buckets", kind="bar", data=master_test_result_df2_viz,dodge=True,legend=True).set(title="Input_week vs RMSE for input=week and model=RandomRegressor");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_week", y="MAE", hue="Buckets", kind="bar", data=master_test_result_df2_viz,dodge=True,legend=True).set(title="Input_week vs MAE for input=week and model=RandomRegressor");

# COMMAND ----------

# MAGIC %md
# MAGIC Calling functions for calculating values for RMSE, MAE using week as a input and baseline model as a model (Running model on 3 weeks)

# COMMAND ----------

# DBTITLE 1,Compute
list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
inp_date_list = ['2021-12-06','2021-12-13','2022-01-10']                         #Date is start day of week
master_test_result_df3 = pd.DataFrame(columns = ['Buckets', 'Total_records', 'RMSE','MAE','inp_date'])

for inp_date in inp_date_list:
    print(inp_date)
    df_inp = baseline_model_df[(baseline_model_df['Inp_date'] == inp_date)]
    rmse_test,mae_test,inactive_predictions = test_model(df_inp['mean_last_3'], df_inp['inactive_days'])    #Calling test model
    forest_rmse_list =[]
    mae_list = []
    data_count_list = []
    
    #Bucketwise calculation
    for i in list1:
        forest_rmse,mae,True_value = bucket_cal(inactive_predictions,df_inp['inactive_days'],i[0],i[1])
        forest_rmse_list.append(forest_rmse)
        mae_list.append(mae)
        data_count_list.append(True_value.count())

    test_result_df = pd.DataFrame(list(zip(list1,data_count_list,forest_rmse_list,mae_list)),columns =['Buckets','Total_records','RMSE','MAE'])
    test_result_df['inp_date']=inp_date
    test_result_df.index= list(range(1, len(test_result_df) +1))
    test_result_df.loc[0] = ['Overall',df_inp['mean_last_3'].count(),rmse_test,mae_test,inp_date]
    test_result_df = test_result_df.sort_index()                           
    master_test_result_df3=master_test_result_df3.append(test_result_df, ignore_index=True)
    
first_column = master_test_result_df3.pop('inp_date')
master_test_result_df3.insert(0, 'inp_date', first_column)
    
print("Overall and bucket wise performance of baseline model by week as input")    
print(master_test_result_df3)

# COMMAND ----------

# DBTITLE 1,Compute
outname = 'master_test_result_df3.csv'
outdir = '/dbfs/FileStore/'
master_test_result_df3.to_csv(outdir+outname, index=False, encoding="utf-8")
master_test_result_df3_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/master_test_result_df3.csv')
display(master_test_result_df3_csv)
master_test_result_df3_viz = master_test_result_df3_csv.toPandas()
inp_display_1 = ['2021-12-6']
inp_display_1 = inp_display_1 * 6
inp_display_2= ['2021-12-13']
inp_display_2 = inp_display_2 * 6
inp_display_3= ['2022-01-10']
inp_display_3 = inp_display_3 * 6
inp_display = inp_display_1 + inp_display_2 + inp_display_3
master_test_result_df3_viz['Input_week'] = inp_display
#Total records in dataset as well as records in each bucket for training data 
sns.catplot(x="Input_week", y="Total_records", hue="Buckets", kind="bar", data=master_test_result_df3_viz,dodge=True,legend=True).set(title="Input_week vs Total_records for input=week and model= baseline").set(ylim=(0, 250000));
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_week", y="RMSE", hue="Buckets", kind="bar", data=master_test_result_df3_viz,dodge=True,legend=True).set(title="Input_week vs RMSE for input=week and model= baseline");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_week", y="MAE", hue="Buckets", kind="bar", data=master_test_result_df3_viz,dodge=True,legend=True).set(title="Input_week vs MAE for input=week model= baseline");

# COMMAND ----------

# MAGIC %md
# MAGIC #####Insights
# MAGIC 1. Day level giving us better metric value compare to week level hence we will create model on day level.

# COMMAND ----------

# MAGIC %md
# MAGIC ###Next-Trade-Day Model
# MAGIC 
# MAGIC - Targeting customers who have *trade_day_count* <4.
# MAGIC 
# MAGIC Features used:
# MAGIC 1.trade_days_cnt
# MAGIC 
# MAGIC 2.total_trades
# MAGIC 
# MAGIC 3.total_order_count
# MAGIC 
# MAGIC 4.gender
# MAGIC 
# MAGIC 5.riskcategory
# MAGIC 
# MAGIC 6.incomedetails
# MAGIC 
# MAGIC 7.occupation
# MAGIC 
# MAGIC 8.age
# MAGIC 
# MAGIC 9.activeinsys
# MAGIC 
# MAGIC 10.city
# MAGIC 
# MAGIC 11.avg_investment
# MAGIC 
# MAGIC 
# MAGIC Target:
# MAGIC Inactive_days=Days difference between DTU date and next trade day

# COMMAND ----------

# DBTITLE 1,Function to create data for input as a day. It calculates data for both training and testing.
#In this function we are removing mean and std_dev as out of 3 one will be DTU and even if we found last 2 trades in last month data
#we can't confirm that this is a consistent behaviour because in past these customers have never traded. 
#Also as there is no mean there won't be any baseline model analysis for trade_day_count < 4
def data_day_less4(inp_date):
    inp_date1 = inp_date
    #Fetching data from database
    df1 = sqlContext.sql(f"""select a.Party_code,trade_days_cnt,total_trades,total_order_count,
                          gender,riskcategory,incomedetails,occupation,age,activeinsys,city,avg_investment,
                          nvl(next_trade,(date_add(cast( '"""+inp_date+"""' as date),65))) as next_trade 
                          from
                      
                         ((select Party_code, count(distinct sauda_date) as trade_days_cnt from 
                          as_ordercountdata where 
                          Party_code in 
                          (select distinct a.Party_code from as_ordercountdata as a 
                          ,SN_ClientKYC as b
                           where b.B2C = 'Y' and a.sauda_date = cast( '"""+inp_date+"""' as date) and a.Party_code = b.Party_Code)
                           and sauda_date between date_add(cast( '"""+inp_date+"""' as date),-30) and cast( '"""+inp_date+"""' as date)
                           group by Party_code having count(distinct sauda_date) < 4) AS a
                       
                            left join
                       
                          (select Party_code, 
                          count(*) as total_trades,
                          sum(OC) as total_order_count,
                          AVG(T_O) AS avg_investment
                          from as_ordercountdata
                          where sauda_date between date_add(cast( '"""+inp_date+"""' as date),-30) and cast( '"""+inp_date+"""' as date)
                          group by party_code) AS  b
                       
                          on a.Party_code = b.Party_code
                       
                          left join
                       
                            (select distinct sn.Party_Code,
                           gender,
                           riskcategory,
                           incomedetails,
                           occupation,
                           DATEDIFF(birthdate, cast( '"""+inp_date+"""' as date))/365 AS age,
                           DATEDIFF(activefrom,cast( '"""+inp_date+"""' as date)) as activeinsys,
                           city
                           FROM SN_ClientKYC as sn 
                          where B2C='Y') AS c
                       
                           on  a.Party_code=c.Party_code
                       
                           left join 
                       
                           (select Party_code, min(sauda_date) as next_trade from 
                          as_ordercountdata where Party_code in 
                          (select distinct a.Party_code from as_ordercountdata as a ,SN_ClientKYC as b
                           where b.B2C = 'Y' and a.sauda_date = cast( '"""+inp_date+"""' as date) and a.Party_code = b.Party_Code)
                           and sauda_date between date_add(cast( '"""+inp_date+"""' as date),1) and  date_add(cast( '"""+inp_date+"""' as date),61)
                           group by Party_code having count(distinct sauda_date) < 4) As d

                       
                           on a.Party_code = d.Party_code) """)
    df=df1.toPandas()
    df1.unpersist()
    #Calculating target variable
    df['next_trade']=pd.to_datetime(df['next_trade'])
    df['date']=inp_date
    df['date'] = pd.to_datetime(df['date'])
    dates1 = df['date'].values.astype('datetime64[D]')
    dates2 = df['next_trade'].values.astype('datetime64[D]')
    
    input = inp_date
    format = '%Y-%m-%d'
    datetime1 = datetime.datetime.strptime(input, format)
    inp_date=datetime1.date()
    
    #Calculate non business days:
    df["daterange"] = df.apply(lambda x: pd.date_range(x.date, x.next_trade), axis=1)
    df["daterange"] = df.apply(lambda x: set(x['daterange'].to_list()), axis=1)
    
    cal_day = pd.date_range(start=inp_date,end=inp_date+timedelta(days=60))
    list1 = cal_day.to_frame()[0].to_list()
    df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between cast( '"""+inp_date1+"""' as date)
                         and date_add(cast( '"""+inp_date1+"""' as date),60))""")
    df_busi_day=df_busi_day1.toPandas()
    list2 = df_busi_day['TradingDay']
    df['next_60_days_holi_cal'] = df.apply(lambda x: set(list1) ^ set(list2),axis=1)
    df["non_business_count"]=df.apply(lambda x: len(x.daterange & x.next_60_days_holi_cal),axis=1)
        
    #Doing this handling as we need to consider only business days   
    df['inactive_days'] = dates2 -dates1
    df['inactive_days'] = df['inactive_days'].dt.days -1 - df["non_business_count"]
    #Calculating default days for missing value    
    default=len(list1)-len(df['next_60_days_holi_cal'][0])
    #Missing value handling
    df['inactive_days'].fillna(60-default, inplace=True)

    #Converting city to tier
    tier_info1 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/Indian_Cities_by_Tier_I_and_II.csv')
    tier_info=tier_info1.toPandas()
    tier_info.dropna(subset=['Tier'], inplace=True)
    tier_mapper = tier_info[['City', 'Tier']].set_index('City').to_dict()['Tier']    
    def tier_mapping(city):
        try: return tier_mapper[city.capitalize()]
        except: return 'Tier-III'
    df['Tier'] = df['city'].map(tier_mapping)
    df = df.drop(['city'], axis = 1)
    #Missing values
    df['occupation'].fillna('other', inplace=True)
    df['riskcategory'].fillna('other', inplace=True)  
    #calculating age buckets
    def f(row):
        if row <=35:
            val = 1
        elif row > 35 and row<=45:
            val = 2
        else:
            val = 3
        return val
    df['new_age'] = df['age'].apply(f)
    #Calculating income buckets
    def f1(row):
        if row =='BELOW 1 LAC':
            val = 1
        elif row == '1-5 LAC':
            val = 2
        elif row == '5-10 LAC':
            val = 3
        elif row == '10-25 LAC':
            val = 4
        elif row == '>25 LAC':
            val = 5
        else:
            val = 0
        return val
    df['new_incomedetails'] = df['incomedetails'].apply(f1)

    df["gender"].replace({"f": "F"}, inplace=True)
    def f2(row):
        if row == 'F':
            val = 1
        else:
            val = 0
        return val
    df['gender_f'] = df['gender'].apply(f2)

    def f3(row):
        if row == 'HIGH':
            val = 2
        elif row == 'MEDIUM':
            val = 1
        else:
            val=0
        return val
    df['riskcategory_new'] = df['riskcategory'].apply(f3)
    df = df.drop(['gender','incomedetails','age','riskcategory','Party_code','next_trade','date','daterange'
                 ,'next_60_days_holi_cal','non_business_count'],axis=1)
    #onehot encoding
    print(df)
    X = df.copy()
    cat_attribs = ['occupation', 'Tier']

    # creating instance of one-hot-encoder
    enc = OneHotEncoder(handle_unknown='ignore')
    enc_df = pd.DataFrame(enc.fit_transform(X[cat_attribs]).toarray())
    #enc_df = pd.DataFrame(enc.transform(X[cat_attribs]).toarray())
    X = X.join(enc_df)
    X= X.drop(['Tier','occupation'],axis=1)
    Y= X['inactive_days']
    X_train,Y_train = X,Y
    X_train = X_train.drop(['inactive_days'], axis=1) 
    X_train.fillna(0, inplace=True)
    return (X_train,Y_train)

# COMMAND ----------

# DBTITLE 1,Function to create data for input as a week. It calculates data for both training and testing.
def data_prep_week_less4(inp_date):
    inp_date1 = inp_date
    #Fetching data from database
    df1 = sqlContext.sql(f"""select a.Party_code,trade_days_cnt,total_trades,total_order_count,
                          gender,riskcategory,incomedetails,occupation,age,activeinsys,city,avg_investment,
                          nvl(next_trade,(date_add(cast( '"""+inp_date+"""' as date),65))) as next_trade,inp_sauda_date 
                          from
                      
                         ((select Party_code, count(distinct sauda_date) as trade_days_cnt from 
                          as_ordercountdata where 
                          Party_code in 
                          (select distinct a.Party_code from as_ordercountdata as a 
                          ,SN_ClientKYC as b
                           where b.B2C = 'Y' and a.sauda_date between cast( '"""+inp_date+"""' as date) and date_add(cast( '"""+inp_date+"""' as date),4) 
                           and a.Party_code = b.Party_Code)
                           
                           and sauda_date between date_add(cast( '"""+inp_date+"""' as date),-30) and cast( '"""+inp_date+"""' as date)
                           group by Party_code having count(distinct sauda_date) < 4 ) AS a
                       
                            left join
                       
                          (select Party_code, 
                          count(*) as total_trades,
                          sum(OC) as total_order_count,
                          AVG(T_O) AS avg_investment
                          from as_ordercountdata
                          where sauda_date between date_add(cast( '"""+inp_date+"""' as date),-30) and cast( '"""+inp_date+"""' as date)
                          group by party_code) AS  b
                       
                          on a.Party_code = b.Party_code
                       
                          left join
                       
                            (select distinct sn.Party_Code,
                           gender,
                           riskcategory,
                           incomedetails,
                           occupation,
                           DATEDIFF(birthdate, cast( '"""+inp_date+"""' as date))/365 AS age,
                           DATEDIFF(activefrom,cast( '"""+inp_date+"""' as date)) as activeinsys,
                           city
                           FROM SN_ClientKYC as sn 
                          where B2C='Y') AS c
                       
                           on  a.Party_code=c.Party_code
                       
                           left join 
                       
                          (select Party_code, min(sauda_date) as next_trade from 
                          as_ordercountdata where Party_code in 
                          (select distinct a.Party_code from as_ordercountdata as a ,SN_ClientKYC as b
                           where b.B2C = 'Y' and a.sauda_date between cast( '"""+inp_date+"""' as date) and date_add(cast( '"""+inp_date+"""' as date),4) 
                           and a.Party_code = b.Party_Code)
                           and sauda_date between date_add(cast( '"""+inp_date+"""' as date),5) and date_add(cast( '"""+inp_date+"""' as date),66)
                           group by Party_code having count(distinct sauda_date) < 4) As d      
                       
                           on a.Party_code = d.Party_code
                           
                           left join 
                       
                           (select Party_code,
                              max(sauda_date) as inp_sauda_date
                           from as_ordercountdata
                           where sauda_date between cast( '"""+inp_date+"""' as date) and date_add(cast( '"""+inp_date+"""' as date),4)
                           group by Party_code) As e
                       
                           on a.Party_code = e.Party_code) """)
    df=df1.toPandas()
    df1.unpersist()
    #Calculating target variable
    df['next_trade']=pd.to_datetime(df['next_trade'])
    df['inp_sauda_date']=pd.to_datetime(df['inp_sauda_date'])
    dates1 = df['inp_sauda_date'].values.astype('datetime64[D]')
    dates2 = df['next_trade'].values.astype('datetime64[D]')
     
    input = inp_date
    format = '%Y-%m-%d'
    datetime1 = datetime.datetime.strptime(input, format)
    inp_date=datetime1.date()  
    
    #Calculate non business days:
    df["daterange"] = df.apply(lambda x: pd.date_range(start= x.inp_sauda_date, end = x.next_trade, freq ='D'), axis=1)
    df["daterange"] = df.apply(lambda x: set(x['daterange'].to_list()), axis=1)
    
    cal_day = pd.date_range(start=inp_date,end=inp_date+timedelta(days=64))
    list1 = cal_day.to_frame()[0].to_list()
    df_busi_day1 = sqlContext.sql(f"""(select TradingDay  from SN_TradingDays where TradingDay between cast( '"""+inp_date1+"""' as date)
                         and date_add(cast( '"""+inp_date1+"""' as date),64))""")
    df_busi_day=df_busi_day1.toPandas()
    list2 = df_busi_day['TradingDay']
    df['next_60_days_holi_cal'] = df.apply(lambda x: set(list1) ^ set(list2),axis=1)
    df["non_business_count"]=df.apply(lambda x: len(x.daterange & x.next_60_days_holi_cal),axis=1)
        
    #Doing this handling as we need to consider only business days   
    df['inactive_days'] = dates2 -dates1
    df['inactive_days'] = df['inactive_days'].dt.days -1 - df["non_business_count"]
    
    #Calculating default days for missing value    
    default=len(list1)-len(df['next_60_days_holi_cal'][0])
    #Missing value handling
    df['inactive_days'].fillna(64-default, inplace=True)

    
    
    #Converting city to tier
    tier_info1 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/Indian_Cities_by_Tier_I_and_II.csv')
    tier_info=tier_info1.toPandas()
    tier_info.dropna(subset=['Tier'], inplace=True)
    tier_mapper = tier_info[['City', 'Tier']].set_index('City').to_dict()['Tier']
    def tier_mapping(city):
        try: return tier_mapper[city.capitalize()]
        except: return 'Tier-III'
    df['Tier'] = df['city'].map(tier_mapping)
    df = df.drop(['city'], axis = 1)
    
    #Missing values
    df['occupation'].fillna('other', inplace=True)
    df['riskcategory'].fillna('other', inplace=True)
    #Calculating age buckets
    def f(row):
        if row <=35:
            val = 1
        elif row > 35 and row<=45:
            val = 2
        else:
            val = 3
        return val
    df['new_age'] = df['age'].apply(f)
    #Calculating income buckets
    def f1(row):
        if row =='BELOW 1 LAC':
            val = 1
        elif row == '1-5 LAC':
            val = 2
        elif row == '5-10 LAC':
            val = 3
        elif row == '10-25 LAC':
            val = 4
        elif row == '>25 LAC':
            val = 5
        else:
            val = 0
        return val
    df['new_incomedetails'] = df['incomedetails'].apply(f1)

    df["gender"].replace({"f": "F"}, inplace=True)
    def f2(row):
        if row == 'F':
            val = 1
        else:
            val = 0
        return val
    df['gender_f'] = df['gender'].apply(f2)

    def f3(row):
        if row == 'HIGH':
            val = 2
        elif row == 'MEDIUM':
            val = 1
        else:
            val=0
        return val
    df['riskcategory_new'] = df['riskcategory'].apply(f3)
    
    df = df.drop(['gender','incomedetails','age','riskcategory','Party_code','next_trade','daterange'
                 ,'next_60_days_holi_cal','non_business_count','inp_sauda_date'],axis=1)
         
    #onehot encoding
    print(df)
    X = df.copy()
    cat_attribs = ['occupation', 'Tier']

    # creating instance of one-hot-encoder
    enc = OneHotEncoder(handle_unknown='ignore')
    enc_df = pd.DataFrame(enc.fit_transform(X[cat_attribs]).toarray())
    #enc_df = pd.DataFrame(enc.transform(X[cat_attribs]).toarray())
    X = X.join(enc_df)
    X= X.drop(['Tier','occupation'],axis=1)
    Y= X['inactive_days']
    X_train,Y_train = X,Y
    X_train = X_train.drop(['inactive_days'], axis=1) 
    X_train.fillna(0, inplace=True)     
    return (X_train,Y_train)


# COMMAND ----------

# MAGIC %md
# MAGIC **Calling functions for calculating values for RMSE, MAE using day as a input and randomregressor as a model**

# COMMAND ----------

# DBTITLE 1,Data preparation for train by taking day as a input
inp_date = '2021-12-01'
print(inp_date)

#Checking if input is valid business day
df_check = sqlContext.sql(f"""select TradingDay from SN_TradingDays where TradingDay=cast( '"""+inp_date+"""' as date)""")
if (df_check.rdd.isEmpty()):
    print("Input date is not a valid business date")
    sys.exit()
else:
    print("Call function")
    df_check.unpersist()
    X_train,Y_train = data_day_less4(inp_date)


# COMMAND ----------

# DBTITLE 1,Training model
print("Calling training model")
rmse_train,MAE,forest_reg,inactive_predictions = train_model(X_train,Y_train)
#Bucketwise calculation for train
list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
forest_rmse_list =[]
mae_list = []
data_count_list = []
for i in list1:
    forest_rmse,mae,True_value = bucket_cal(inactive_predictions,Y_train,i[0],i[1])
    forest_rmse_list.append(forest_rmse)
    mae_list.append(mae)
    data_count_list.append(True_value.count())

print("Overall and Bucketwise training performance for day as input and randomregressor as a model for trade_day_count < 4")
train_result_df_less4 = pd.DataFrame(list(zip(list1,data_count_list,forest_rmse_list,mae_list)),columns =['Buckets','Total_records','RMSE','MAE'])
train_result_df_less4['inp_date']=inp_date
train_result_df_less4.index= list(range(1, len(train_result_df_less4) +1))
train_result_df_less4.loc[0] = ['Overall',Y_train.count(),rmse_train,MAE,inp_date]
train_result_df_less4 = train_result_df_less4.sort_index()
first_column = train_result_df_less4.pop('inp_date')
train_result_df_less4.insert(0, 'inp_date', first_column)
print(train_result_df_less4)

# COMMAND ----------

# DBTITLE 1,Compute
outname = 'train_result_df_less4.csv'
outdir = '/dbfs/FileStore/'
train_result_df_less4.to_csv(outdir+outname, index=False, encoding="utf-8")
train_result_df_less4_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/train_result_df_less4.csv')
display(train_result_df_less4_csv)
df_train_less4=train_result_df_less4_csv.toPandas()
inp_display20 = ['2021-12-01'] * 6
df_train_less4['Input_date'] = inp_display20
#Total records in dataset as well as records in each bucket for training data
sns.catplot(x="Input_date", y="Total_records", hue="Buckets", kind="bar", data=df_train_less4,dodge=True,legend=True).set(title="Train Input_date vs Total_records for input=day, model=Randomregressor and trade_day_count<4");
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_date", y="RMSE", hue="Buckets", kind="bar", data=df_train_less4,dodge=True,legend=True).set(title="Train Input_date vs RMSE for input=day and model=Randomregressor and trade_day_count<4");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_date", y="MAE", hue="Buckets", kind="bar", data=df_train_less4,dodge=True,legend=True).set(title="Train Input_date vs MAE for input=day and model=Randomregressor and trade_day_count<4");

# COMMAND ----------

# DBTITLE 1,Testing randomregressor model by taking day as input (Testing on 3 dates)
inp_date_list = ['2021-12-02','2021-12-03','2021-12-08']
master_test_result_df_less4 = pd.DataFrame(columns = ['Buckets', 'Total_records', 'RMSE','MAE','inp_date'])

for inp_date in inp_date_list:
    print(inp_date)
    #Checking if date is valid business day
    df_check = sqlContext.sql(f"""select TradingDay from SN_TradingDays where TradingDay=cast( '"""+inp_date+"""' as date)""")
    if (df_check.rdd.isEmpty()):
        print("Input date is not a valid business date")
        sys.exit()
    else:
        df_check.unpersist()
        X_test,Y_test = data_day_less4(inp_date)
        
    #Calling test model    
    rmse_test,mae_test,inactive_predictions = test_model(X_test, Y_test,forest_reg)

    #bucketwise calculation for test
    list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
    rmse_bucket_list =[]
    mae_bucket_list = []
    bucket_count_list = []
    for i in list1:
        forest_rmse,mae,True_value = bucket_cal(inactive_predictions,Y_test,i[0],i[1])
        rmse_bucket_list.append(forest_rmse)
        mae_bucket_list.append(mae)
        bucket_count_list.append(True_value.count())
    test_result_df_less4 = pd.DataFrame(list(zip(list1,bucket_count_list,rmse_bucket_list,mae_bucket_list)),columns =['Buckets','Total_records','RMSE','MAE'])
    test_result_df_less4['inp_date']=inp_date
    test_result_df_less4.index= list(range(1, len(test_result_df_less4) +1))
    test_result_df_less4.loc[0] = ['Overall',Y_test.count(),rmse_test,mae_test,inp_date]
    test_result_df_less4 = test_result_df_less4.sort_index()
    master_test_result_df_less4=master_test_result_df_less4.append(test_result_df_less4, ignore_index=True)

first_column = master_test_result_df_less4.pop('inp_date')
master_test_result_df_less4.insert(0, 'inp_date', first_column)

print("Overall and bucket wise performance of Test data by using randomregressor as model and day as input for customers with trade_day_count < 4")    
print(master_test_result_df_less4)


# COMMAND ----------

# DBTITLE 1,Compute
outname = 'master_test_result_df_less4.csv'
outdir = '/dbfs/FileStore/'
master_test_result_df_less4.to_csv(outdir+outname, index=False, encoding="utf-8")
master_test_result_df_less4_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/master_test_result_df_less4.csv')
display(master_test_result_df_less4_csv)
master_test_result_df_less4_viz=master_test_result_df_less4_csv.toPandas()
inp_display_11= ['2021-12-02']
inp_display_11 = inp_display_11 * 6
inp_display_12= ['2021-12-03']
inp_display_12 = inp_display_12 * 6
inp_display_13= ['2021-12-08']
inp_display_13 = inp_display_13 * 6
inp_date_display =  inp_display_11 + inp_display_12 + inp_display_13
master_test_result_df_less4_viz['Input_date'] = inp_date_display
#Total records in dataset as well as records in each bucket for training data
sns.catplot(x="Input_date", y="Total_records", hue="Buckets", kind="bar", data=master_test_result_df_less4_viz,dodge=True,legend=True).set(title="Test Input_date vs Total_records for input=day and model=Randomregressor and trade_day_count<4");
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_date", y="RMSE", hue="Buckets", kind="bar", data=master_test_result_df_less4_viz,dodge=True,legend=True).set(title="Test Input_date vs RMSE for input=day and model=Randomregressor and trade_day_count<4");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_date", y="MAE", hue="Buckets", kind="bar", data=master_test_result_df_less4_viz,dodge=True,legend=True).set(title="Test Input_date vs MAE for input=day and model=Randomregressor and trade_day_count<4");

# COMMAND ----------

# MAGIC %md
# MAGIC **Calling functions for calculating values for RMSE, MAE using week as a input and randomregressor as a model**

# COMMAND ----------

# DBTITLE 1,Data preparation for train by taking week as a input
inp_date = '2021-12-06'   #Input is first day of week 2021-12-06 to 2021-12-10
print(inp_date)
#Checking if input date is valid date
df_check = sqlContext.sql(f"""select TradingDay from SN_TradingDays where TradingDay=cast( '"""+inp_date+"""' as date)""")
if (df_check.rdd.isEmpty()):
    print("Input date is not a valid business date")
    sys.exit()
else:
    print("Call function")
    df_check.unpersist()
    X_train,Y_train = data_prep_week_less4(inp_date)


# COMMAND ----------

# DBTITLE 1,Training model
print("Calling training model")
rmse_train,MAE,forest_reg,inactive_predictions = train_model(X_train,Y_train)
#Bucketwise calculation for train
list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
forest_rmse_list =[]
mae_list = []
data_count_list = []

#Calculating bucketwise values
for i in list1:
    forest_rmse,mae,True_value = bucket_cal(inactive_predictions,Y_train,i[0],i[1])
    forest_rmse_list.append(forest_rmse)
    mae_list.append(mae)
    data_count_list.append(True_value.count())
print("Overall and Bucketwise training performance")
train_result_df1_less4 = pd.DataFrame(list(zip(list1,data_count_list,forest_rmse_list,mae_list)),columns =['Buckets','Total_records','RMSE','MAE'])
train_result_df1_less4['inp_date']=inp_date
train_result_df1_less4.index= list(range(1, len(train_result_df1_less4) +1))
train_result_df1_less4.loc[0] = ['Overall',Y_train.count(),rmse_train,MAE,inp_date]
train_result_df1_less4 = train_result_df1_less4.sort_index()
first_column = train_result_df1_less4.pop('inp_date')
train_result_df1_less4.insert(0, 'inp_date', first_column)
print(train_result_df1_less4)

# COMMAND ----------

# DBTITLE 1,compute
outname = 'train_result_df1_less4.csv'
outdir = '/dbfs/FileStore/'
train_result_df1_less4.to_csv(outdir+outname, index=False, encoding="utf-8")
train_result_df1_less4_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/train_result_df1_less4.csv')
display(train_result_df1_less4_csv)
train_result_df1_less4_viz = train_result_df1_less4_csv.toPandas()
inp_display4 = ['2021-12-06'] * 6
train_result_df1_less4_viz['Input_week'] = inp_display4
#Total records in dataset as well as records in each bucket for training data 
sns.catplot(x="Input_week", y="Total_records", hue="Buckets", kind="bar", data=train_result_df1_less4_viz,dodge=True,legend=True).set(title="Train Input_week vs Total_records for input=week ,model=RandomRegressor, trade_day_count < 4 ");
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_week", y="RMSE", hue="Buckets", kind="bar", data=train_result_df1_less4_viz,dodge=True,legend=True).set(title="Train Input_week vs RMSE for input=week and model=RandomRegressor, trade_day_count<4");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_week", y="MAE", hue="Buckets", kind="bar", data=train_result_df1_less4_viz,dodge=True,legend=True).set(title="Train Input_week vs MSAE for input=week  model=RandomRegressor and trade_day_count <4" );

# COMMAND ----------

# DBTITLE 1,Data preparation and Testing of model (Testing on 2 weeks)
inp_date_list = ['2021-12-13','2022-01-10']                     #Date is start day of week
master_test_result_df2_week_less4 = pd.DataFrame(columns = ['Buckets', 'Total_records', 'RMSE','MAE','inp_date'])

for inp_date in inp_date_list:
    print(inp_date)
    df_check = sqlContext.sql(f"""select TradingDay from SN_TradingDays where TradingDay=cast( '"""+inp_date+"""' as date)""")
    if (df_check.rdd.isEmpty()):
        print("Input date is not a valid business date")
        sys.exit()
    else:
        df_check.unpersist()
        X_test,Y_test = data_prep_week_less4(inp_date)
       
    rmse_test,mae_test,inactive_predictions = test_model(X_test, Y_test,forest_reg)             #Testing model

    #bucketwise calculation for test
    list1 = [[0,2],[3,5],[6,10],[11,39],[40,50]]
    rmse_bucket_list =[]
    mae_bucket_list = []
    bucket_count_list = []

    for i in list1:
        forest_rmse,mae,True_value = bucket_cal(inactive_predictions,Y_test,i[0],i[1])
        rmse_bucket_list.append(forest_rmse)
        mae_bucket_list.append(mae)
        bucket_count_list.append(True_value.count())
    test_result_df_less4 = pd.DataFrame(list(zip(list1,bucket_count_list,rmse_bucket_list,mae_bucket_list)),columns =['Buckets','Total_records','RMSE','MAE'])
    test_result_df_less4['inp_date']=inp_date
    test_result_df_less4.index= list(range(1, len(test_result_df_less4) +1))
    test_result_df_less4.loc[0] = ['Overall',Y_test.count(),rmse_test,mae_test,inp_date]
    test_result_df_less4 = test_result_df_less4.sort_index()
    master_test_result_df2_week_less4=master_test_result_df2_week_less4.append(test_result_df_less4, ignore_index=True)

first_column = master_test_result_df2_week_less4.pop('inp_date')
master_test_result_df2_week_less4.insert(0, 'inp_date', first_column)

print("Overall and bucket wise performance of randomregressor model by taking week as a input for customers with trade_day_count < 4")    
print(master_test_result_df2_week_less4)


# COMMAND ----------

# DBTITLE 1,Compute
outname = 'master_test_result_df2_week_less4.csv'
outdir = '/dbfs/FileStore/'
master_test_result_df2_week_less4.to_csv(outdir+outname, index=False, encoding="utf-8")
master_test_result_df2_less4_csv = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/master_test_result_df2_week_less4.csv')
display(master_test_result_df2_less4_csv)
master_test_result_df2_less4_viz = master_test_result_df2_less4_csv.toPandas()
inp_display_21= ['2021-12-13']
inp_display_21 = inp_display_21 * 6
inp_display_22= ['2022-01-10']
inp_display_22 = inp_display_22 * 6
inp_display = inp_display_21 + inp_display_22
master_test_result_df2_less4_viz['Input_week'] = inp_display
#Total records in dataset as well as records in each bucket for training data 
sns.catplot(x="Input_week", y="Total_records", hue="Buckets", kind="bar", data=master_test_result_df2_less4_viz,dodge=True,legend=True).set(title="Input_week vs Total_records for input=week,model=RandomRegressor, trade_day_count<4");
#Total RMSE of dataset as well as RMSE in each bucket for training data
sns.catplot(x="Input_week", y="RMSE", hue="Buckets", kind="bar", data=master_test_result_df2_less4_viz,dodge=True,legend=True).set(title="Input_week vs RMSE for input=week and model=RandomRegressor, trade_day_count<4");
#Total MAE of dataset as well as MAE for each bucket 
sns.catplot(x="Input_week", y="MAE", hue="Buckets", kind="bar", data=master_test_result_df2_less4_viz,dodge=True,legend=True).set(title="Input_week vs MAE for input=week and model=RandomRegressor,trade_day_ount<4");

# COMMAND ----------

# MAGIC %md
# MAGIC #####Insights 
# MAGIC 
# MAGIC -RMSE/MAE is high for trade_day_count < 4 compared to trade_day_count >= 4
# MAGIC 
# MAGIC -There is no consistent behaviour between buckets for both day and week (Bucket size is not decreasing as inactive days are increasing)
# MAGIC 
# MAGIC -There is no relation between bucket size and RMSE/MAE values (As bucket size decreases, RMSE should get increased )

# COMMAND ----------

# DBTITLE 0,I
# MAGIC %md
# MAGIC 
# MAGIC ###Conclusion 
# MAGIC 
# MAGIC **After performing 6 Exploratory Data Analysis**
# MAGIC 
# MAGIC 1. Based on first trade and number of trades performed in system we can find if customer is stable in system. Stable customers have low probability of getting dormant.
# MAGIC 2. As discussed in EDA 3 we can say that we have 2 segments while developing model one with trade_day_count >= 4 and second with trade_day_count < 4.
# MAGIC 3. Frequency and Recency plays important role in predicting dormancy hence we will train only past one month data.
# MAGIC 4. As discussed in EDA3 we will have 'inactive_days' as target variable.
# MAGIC 
# MAGIC **Below are valid only for customers with trade_day_count >= 4**
# MAGIC 
# MAGIC 5. In all models day input has better RMSE over week input
# MAGIC 6. In all models,RMSE increasing per group in ascending order. 
# MAGIC 7. Hence,We can keep granular level as day and metric as RMSE.
# MAGIC 8. ML model performs better than baseline model.
# MAGIC 9. customer with 0 mean likely to have next inactive duration as 0 and customer with more than 30 inactive days likely to get dormant so can offer them offers related dormancy.
# MAGIC 10. Hence, We can build separate model/process for customer with 0 mean/std deviation and customer with more than 30 inactive days.

# COMMAND ----------

# MAGIC %md
# MAGIC ###Call for Action
# MAGIC - **In its current form, we need more clarity on business problem and the problem sizing**
# MAGIC - **Once we have clarity on the sizing, we can frame the problem as a Learning problem**
# MAGIC - **Till then we can pivot and formalise the analysis into an active insight dashboard**

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC #Next steps after presenting above task to revenue team:

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC a. Consider customers who performed their first trade, predict if they will have sauda_day_count < 4
# MAGIC 
# MAGIC b. Consider customers who has sauda_day_count < 4, predict their probablity of having sauda_day_count >= 4
# MAGIC 
# MAGIC c. Consider customers who has sauda_day_count >=4 predict their next purchase bucket t, buckets will be:  week < t <= 2 week,  2 week < t <= month, month < t <= quarter
# MAGIC 
# MAGIC d. Create data mart and visualization of data flow in  user state diagram.
# MAGIC 
# MAGIC ***Note: In subsequent phases of project we won't be predicting dormancy rather we will be predicting state of user and based on that revenue team will decide on offers and 
# MAGIC rewards for customer. Hence title of project changed to 'offers and rewards allocation engine'.***

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ***Thank you!***
